{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Splitting in Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'clip.pdf', 'page': 0}, page_content='Published as a conference paper at ICLR 2024\\nANOMALY CLIP: O BJECT -AGNOSTIC PROMPT LEARN -\\nING FOR ZERO-SHOT ANOMALY DETECTION\\nQihang Zhou1∗, Guansong Pang2∗, Yu Tian3, Shibo He1†, Jiming Chen1†\\n1Zhejiang University2Singapore Management University3Harvard University\\n1{zqhang, s18he, cjm }@zju.edu.cn2gspang@smu.edu.sg\\n3ytian11@meei.harvard.edu\\nABSTRACT\\nZero-shot anomaly detection (ZSAD) requires detection models trained using aux-\\niliary data to detect anomalies without any training sample in a target dataset. It\\nis a crucial task when training data is not accessible due to various concerns, e.g.,\\ndata privacy, yet it is challenging since the models need to generalize to anoma-\\nlies across different domains where the appearance of foreground objects, abnor-\\nmal regions, and background features, such as defects/tumors on different prod-\\nucts/organs, can vary significantly. Recently large pre-trained vision-language\\nmodels (VLMs), such as CLIP, have demonstrated strong zero-shot recognition\\nability in various vision tasks, including anomaly detection. However, their ZSAD\\nperformance is weak since the VLMs focus more on modeling the class semantics\\nof the foreground objects rather than the abnormality/normality in the images.\\nIn this paper we introduce a novel approach, namely AnomalyCLIP, to adapt\\nCLIP for accurate ZSAD across different domains. The key insight of Anoma-\\nlyCLIP is to learn object-agnostic text prompts that capture generic normality and\\nabnormality in an image regardless of its foreground objects. This allows our\\nmodel to focus on the abnormal image regions rather than the object semantics,\\nenabling generalized normality and abnormality recognition on diverse types of\\nobjects. Large-scale experiments on 17 real-world anomaly detection datasets\\nshow that AnomalyCLIP achieves superior zero-shot performance of detecting\\nand segmenting anomalies in datasets of highly diverse class semantics from vari-\\nous defect inspection and medical imaging domains. Code will be made available\\nathttps://github.com/zqhang/AnomalyCLIP .\\n1 I NTRODUCTION\\nAnomaly detection (AD) has been widely applied in various applications, such as industrial defect\\ninspection (Bergmann et al., 2019; Xie et al., 2023; Roth et al., 2022; Huang et al., 2022; Mou\\net al., 2022; Chen et al., 2022; Bergmann et al., 2020; Pang et al., 2021a; Reiss & Hoshen, 2023;\\nYou et al., 2022; Liznerski et al., 2020; Ding et al., 2022; Cao et al., 2023) and medical image\\nanalysis (Pang et al., 2021a; Qin et al., 2022; Liu et al., 2023; Ding et al., 2022; Tian et al., 2021;\\n2023; Fernando et al., 2021). Existing AD approaches typically assume that training examples in a\\ntarget application domain are available for learning the detection models (Pang et al., 2021b; Ruff\\net al., 2021). However, this assumption may not hold in various scenarios, such as i) when accessing\\ntraining data violates data privacy policies ( e.g., to protect the sensitive information of patients), or\\nii) when the target domain does not have relevant training data ( e.g., inspecting defects in a manu-\\nfacturing line of new products). Zero-shot anomaly detection (ZSAD) is an emerging task for AD\\nin such scenarios, to which the aforementioned AD approaches are not viable, as it requires detec-\\ntion models to detect anomalies without any training sample in a target dataset. Since anomalies\\nfrom different application scenarios typically have substantial variations in their visual appearance,\\nforeground objects, and background features, e.g., defects on the surface of one product vs. that\\non the other products, lesions/tumors on different organs, or industrial defects vs. tumors/lesions\\nin medical images, detection models with strong generalization ability w.r.t. such variations are\\n∗Equal contribution. †Corresponding authors.\\n1arXiv:2310.18961v8  [cs.CV]  16 Mar 2024'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 1}, page_content='Published as a conference paper at ICLR 2024\\nCapsule \\n(MVTec AD)Metal nut \\n(MVTec AD)\\nTransistor\\n(MVTec AD)Tile\\n(MVTec AD)\\nLeather\\n(MVTec AD)Carpet\\n(MVTec AD)Auxiliary data\\n(a)\\nClass1\\n(DAGM)Colon\\n(ColonDB )PCB\\n(Visa)Skin\\n(ISIC)Metal plate\\n(Visa)Brian\\n(Br35H)Test data (b)\\nOriginal  text prompts\\nSimilarity map\\n（CLIP ）\\n (c)\\nSimilarity map\\n（WinCLIP ）Tailored text prompts (d)\\nSimilarity map\\n（CoOp）Learnable text prompts (e)\\nSimilarity map\\n（AnomalyCLIP ）Object -agnostic text \\nprompts (f)\\nFigure 1: Comparison of ZSAD results on (b) test data using (c) original text prompts in CLIP\\n(Radford et al., 2021), (d) tailored text prompts for AD in WinCLIP (Jeong et al., 2023), (e) learnable\\ntext prompts for general vision tasks in CoOp (Zhou et al., 2022a), and (f) object-agnostic text\\nprompts in our AnomalyCLIP. (a) presents a set of auxiliary data we can use to learn the text prompts.\\nThe results are obtained by measuring the similarity between text prompt embeddings and image\\nembeddings. The ground-truth anomaly regions are circled in red in (a) and (b). (c), (d), and (e)\\nsuffer from poor generalization across different domains, while our AnomalyCLIP in (f) can well\\ngeneralize to anomalies in diverse types of objects from different domains.\\nneeded for accurate ZSAD. Recently large pre-trained vision-language models (VLMs) (Radford\\net al., 2021; Kirillov et al., 2023) have demonstrated strong zero-shot recognition ability in various\\nvision tasks, including anomaly detection (Jeong et al., 2023). Particularly, being pre-trained us-\\ning millions/billions of image-text pairs, CLIP (Radford et al., 2021) has been applied to empower\\nvarious downstream tasks (Zhou et al., 2022b; Rao et al., 2022; Khattak et al., 2023; Sain et al.,\\n2023) with its strong generalization capability. WinCLIP (Jeong et al., 2023) is a seminal work in\\nthe ZSAD line, which designs a large number of artificial text prompts to exploit the CLIP’s gener-\\nalizability for ZSAD. However, the VLMs such as CLIP are primarily trained to align with the class\\nsemantics of foreground objects rather than the abnormality/normality in the images, and as a result,\\ntheir generalization in understanding the visual abnormality/normality is restricted, leading to weak\\nZSAD performance. Further, the current prompting approaches, using either manually defined text\\nprompts (Jeong et al., 2023) or learnable prompts (Sun et al., 2022; Zhou et al., 2022a), often result\\nin prompt embeddings that opt for global features for effective object semantic alignment (Zhong\\net al., 2022; Wu et al., 2023), failing to capture the abnormality that often manifests in fine-grained,\\nlocal features, as shown in Fig. 1d and Fig. 1e. In this paper we introduce a novel approach, namely\\nAnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. AnomalyCLIP aims to\\nlearn object-agnostic text prompts that capture generic normality and abnormality in an image re-\\ngardless of its foreground objects. It first devises a simple yet universally-effective learnable prompt\\ntemplate for the two general classes – normality and abnormality – and then utilizes both image-level\\nand pixel-level loss functions to learn the generic normality and abnormality globally and locally in\\nour prompt embeddings using auxiliary data. This allows our model to focus on the abnormal im-\\nage regions rather the object semantics, enabling remarkable zero-shot capability of recognizing the\\nabnormality that has similar abnormal patterns to those in auxiliary data. As shown in Fig. 1a and\\nFig. 1b, the foreground object semantics can be completely different in the fine-tuning auxiliary data\\nand target data, but the anomaly patterns remain similar, e.g., scratches on metal nuts and plates, the\\nmisplacement of transistors and PCB, tumors/lesions on various organ surfaces, etc. Text prompt\\nembeddings in CLIP fail to generalize across different domains, as illustrated in Fig. 1c, but object-\\nagnostic prompt embeddings learned by AnomalyCLIP can effectively generalize to recognize the\\nabnormality across different domain images in Fig. 1f.\\nIn summary, this paper makes the following main contributions.\\n• We reveal for the first time that learning object-agnostic text prompts of normality and\\nabnormality is a simple yet effective approach for accurate ZSAD. Compared to current\\ntext prompting approaches that are primarily designed for object semantic alignment (Jeong\\n2'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 2}, page_content='Published as a conference paper at ICLR 2024\\net al., 2023; Zhou et al., 2022b), our text prompt embeddings model semantics of generic\\nabnormality and normality, allowing object-agnostic, generalized ZSAD performance.\\n• We then introduce a novel ZSAD approach, called AnomalyCLIP, in which we utilize an\\nobject-agnostic prompt template and a glocal abnormality loss function (i.e., a combination\\nof global and local loss functions) to learn the generic abnormality and normality prompts\\nusing auxiliary data. In doing so, AnomalyCLIP largely simplifies the prompt design and\\ncan effectively apply to different domains without requiring any change on its learned two\\nprompts, contrasting to existing methods like WinCLIP whose effectiveness relies heavily\\non extensive engineering on hundreds of manually defined prompts.\\n• Comprehensive experiments on 17 datasets from various industrial and medical domains\\ndemonstrate that AnomalyCLIP achieves superior ZSAD performance of detecting and\\nsegmenting anomalies in datasets of highly diverse class semantics from defect inspection\\nand medical imaging domains.\\n2 P RELIMINARY\\nCLIP consists of a text encoder and visual encoder denoted as T(·)andF(·), respectively. Both\\nencoders are mainstream multi-layer networks such as ViT (Dosovitskiy et al., 2020; Vaswani et al.,\\n2017). Using text prompts is a typical way to achieve the embeddings of different classes for zero-\\nshot recognition. Particularly, a text prompt template Gwith the class name ccan be passed through\\nT(·)to obtain its corresponding textual embedding gc∈RD. The text prompt template commonly\\nused in CLIP looks like A photo of a [cls] , where [cls] represents the target class name.\\nThen F(·)encodes an image xito derive visual representations, where the class token fi∈RD\\nis treated as its visual embedding (global visual embedding), and patch tokens fm\\ni∈RH×W×D\\nare referred to as local visual embeddings. CLIP performs zero-shot recognition by measuring the\\nsimilarity between textual and visual embeddings. In specific, given a target class set Cand an image\\nxi, CLIP predicts the probability of xibelonging to cas follows:\\np(y=c|xi) =P(gc, fi) =exp(< gc, fi> /τ)P\\nc∈Cexp(< gc, fi>)/τ), (1)\\nwhere τis a temperature hyperparameter, and the operator <·,·>represents the computation of\\ncosine similarity. Unlike many vision tasks that involve many objects and use the name of the ob-\\njects as the class name [cls] , we posit that performing ZSAD tasks using CLIP should be object-\\nagnostic, so we propose to design two classes of text prompts ( i.e., normality and abnormality) and\\ncompute the possibility of these two classes according to Eq. 1. We denote the probability of being\\nabnormal P(ga, fi)as the anomaly score. The computation is extended from global visual embed-\\ndings to local visual embeddings to derive the corresponding segmentation maps Sn∈RH×Wand\\nSa∈RH×W, where each entry (j, k)are computed as P(gn, fm(j,k)\\ni )andP(ga, fm(j,k)\\ni ).\\n3 A NOMALY CLIP: OBJECT -AGNOSTIC PROMPT LEARNING\\n3.1 A PPROACH OVERVIEW\\nIn this paper, we propose AnomalyCLIP to adapt CLIP to ZSAD via object-agnostic prompt learn-\\ning. As shown in Fig. 2, AnomalyCLIP first introduces object-agnostic text prompt templates, where\\nwe design two generic object-agnostic text prompt templates of gnandgato learn generalized em-\\nbedding for the normality and abnormality classes, respectively (see Sec. 3.2). To learn such generic\\ntext prompt templates, we introduce global and local context optimization to incorporate global and\\nfine-grained anomaly semantics into object-agnostic textual embedding learning. In addition, tex-\\ntual prompt tuning and DPAM are used to support the learning in the textual and local visual spaces\\nof CLIP. Finally, we integrate the multiple intermediate layers to provide more local visual details.\\nDuring training, all modules are jointly optimized by the combination of global and local context\\noptimization. During inference, we quantify the misalignment of textual and global/local visual\\nembeddings to obtain the anomaly score and anomaly score map, respectively (see Sec. 3.3).\\n3'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 3}, page_content='Published as a conference paper at ICLR 2024\\nSimilarity\\nscore\\n…\\nLayer N\\nText encoderMLglobal\\nVisual embeddingLocal visual embedding\\nAuxiliary \\nimageLlocal\\n…\\nLayer 1\\n……\\nLayer 2\\n…\\nTextual embedding\\nGround truth\\nLayer DPAM layer Cosine similarity Element -wise sum MMaximumLearnable\\nFrozenSimilarity map…V1 object VE\\n… damaged object W1WEgnNormal text prompt\\ngaAnomalous text prompt\\nVision encoder...Layer  \\nLayer \\nBlock 1...\\n... Layer  \\nLayer \\nBlock 1...\\nLayer  \\nLayer \\nBlock 2...\\nLayer  \\nLayer \\nBlock 4... Layer  \\nLayer \\nBlock 4...\\nLayer Original layerLayer  \\nLayer \\nBlock 2...Object -agnostic learnable text prompt...\\nFigure 2: Overview of AnomalyCLIP. To adapt CLIP to ZSAD, AnomalyCLIP introduces object-\\nagnostic text prompt templates to capture generic normality and abnormality regardless of the object\\nsemantics. Then, we introduce glocal context optimization to incorporate global and fine-grained\\nanomaly semantics into object-agnostic text prompt learning. Finally, textual prompt tuning and\\nDPAM are used to enable the prompt learning in the textual and local visual spaces of CLIP.\\n3.2 O BJECT -AGNOSTIC TEXT PROMPT DESIGN\\nCommonly used text prompt templates in CLIP, like A photo of a [cls] , primarily focus\\non object semantics. Consequently, they fail to generate textual embeddings that capture anomaly\\nand normal semantics to query corresponding visual embeddings. To support the learning of\\nanomaly-discriminative textual embeddings, we aim to incorporate prior anomaly semantics into\\ntext prompt templates. A trivial solution is to design the templates with specific anomaly types, such\\nasA photo of a [cls] with scratches However, the pattern of anomaly is typically\\nunknown and diverse, so it is practically difficult to list all possible anomaly types. Therefore, it\\nis important to define text prompt templates with generic anomaly semantics. For this purpose, we\\ncan adopt the text damaged [cls] to cover comprehensive anomaly semantics, facilitating the\\ndetection of diverse defects such as scratches and holes. Nevertheless, utilizing such text prompt\\ntemplates poses challenges in generating generic anomaly-discriminating textual embeddings. This\\nis because CLIP’s original pre-training focuses on aligning with object semantics instead of the ab-\\nnormality and normality within images. To address this limitation, we can introduce learnable text\\nprompt templates and tune the prompts using auxiliary AD-relevant data. During the fine-tuning\\nprocess, these learnable templates can incorporate both broad and detailed anomaly semantics, re-\\nsulting in textual embeddings that are more discriminative between normality and abnormality. This\\nhelps avoid the need for manually defined text prompt templates that require extensive engineer-\\ning (Jeong et al., 2023). These text prompts are referred to as object-aware text prompt templates\\nand defined as follows:\\ngn= [V1][V2]. . .[VE][cls]\\nga= [W1][W2]. . .[WE][damaged ][cls],\\nwhere [V]iand[W]i(i∈1, . . . , E ) are learnable word embeddings in normality and abnormality\\ntext prompt templates, respectively.\\nZSAD tasks require models to detect anomalies in previously unseen target datasets. These datasets\\noften exhibit significant variations in object semantics among different objects, like various defects\\n4'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 4}, page_content='Published as a conference paper at ICLR 2024\\non one product vs. another, or discrepancies between industrial defects and medical imaging tu-\\nmors. However, despite these substantial differences in object semantics, the underlying anomaly\\npatterns could be similar. For instance, anomalies like scratches on metal nuts and plates, or the\\nmisplacement of transistors and PCB, as well as tumors on the surface of various organs, can share\\nsimilar anomaly patterns. We hypothesize that the key of accurate ZSAD is to identify these generic\\nanomaly patterns regardless of the varying semantics of different objects. Therefore, the inclusion of\\nobject semantics in object-aware text prompt templates is often unnecessary for ZSAD. It can even\\nhinder the detection of anomalies in classes that have not been seen during the learning process.\\nMore importantly, excluding the object semantics from text prompt templates allows learnable text\\nprompt templates to focus on capturing the characteristics of anomalies themselves, rather than the\\nobjects. Motivated by this, we introduce object-agnostic prompt learning, with the aim to capture\\ngeneric normality and abnormality within images regardless of the object semantics. Different from\\nobject-aware text prompt templates, as shown below, the object-agnostic text prompt templates\\nreplace the class name in gnandgawithobject , blocking out the class semantics of objects:\\ngn= [V1][V2]. . .[VE][object ]\\nga= [W1][W2]. . .[WE][damaged ][object ].\\nThis design empowers the object-agnostic text prompt template to learn the shared patterns of dif-\\nferent anomalies. As a result, the generated textual embeddings are more generic and capable of\\nidentifying anomalies across diverse objects and different domains. Further, this prompt design is\\nversatile and can be applied to different target domains without any modification, e.g., requiring no\\nknowledge about the object name or anomaly types in a target dataset.\\n3.3 L EARNING GENERIC ABNORMALITY AND NORMALITY PROMPTS\\nGlocal context optimization To effectively learn the object-agnostic text prompts, we devise a\\njoint optimization approach that enables the normality and abnormality prompt learning from both\\nglobal and local perspectives, namely global and local context optimization. The global context op-\\ntimization aims to enforce that our object-agnostic textual embeddings are matched with the global\\nvisual embeddings of images of diverse objects. This helps effectively capture the normal/abnormal\\nsemantics from a global feature perspective. The local context optimization is introduced to enable\\nobject-agnostic text prompts to concentrate on fine-grained, local abnormal regions from Minter-\\nmediate layers of the visual encoder, in addition to the global normal/abnormal features. Formally,\\nletMbe the set of intermediate layers used ( i.e.,M=|M|), our text prompts are learned by\\nminimizing the following glocal loss function:\\nLtotal=Lglobal +λP\\nMk∈MLMk\\nlocal, (2)\\nwhere λis a hyperparameter to balance the global and local losses. Lglobal is a cross-entropy loss\\nthat matches the cosine similarity between the object-agnostic textual embeddings and visual embed-\\ndings of normal/abnormal images from auxiliary data. Let S∈RHimage×Wimage be the ground-truth\\nsegmentation mask, with Sjk= 1if the pixel is as an anomaly and Sjk= 0otherwise, then we have\\nS(j,k)\\nn,Mk=P(gn, fm(j,k)\\ni,Mk), S(j,k)\\na,Mk=P(ga, fm(j,k)\\ni,Mk),where j∈[1, H], k∈[1, W]\\nLlocal=Focal (Up([Sn,Mk, Sa,Mk]), S) +Dice(Up(Sn,Mk), I−S) +Dice(Up(Sa,Mk), S),\\nwhere Focal (·,·)andDice(·,·)denote a focal loss (Lin et al., 2017) and a Dice loss (Li et al., 2019)\\nrespectively. The operators Up(·)and[·,·]represent the unsampling and concatenation along with\\nthe channel, and Irepresents the full-one matrix. Since the anomalous regions are typically smaller\\nthan the normal ones, we use focal loss to address the imbalance problem. Furthermore, to ensure\\nthat the model establishes an accurate decision boundary, we employ the Dice loss to measure the\\noverlaps between the predicted segmentation Up(Sn,Mk)/Up(Sa,Mk)and the ground truth mask.\\nRefinement of the textual space To facilitate the learning of a more discriminative textual space\\nvia Eq. 2, inspired by Jia et al. (2022) and Khattak et al. (2023), we introduce text prompt tuning to\\nrefine the original textual space of CLIP by adding additional learnable token embeddings into its\\ntext encoder. Specifically, we first attach randomly initialized learnable token embeddings t′\\nminto\\nTm, them-th layer of the frozen CLIP text encoder. Then, we concatenate t′\\nmand the original token\\nembeddings tmalong the dimension of the channel, and forward them to Tmto get the corresponding\\n5'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 5}, page_content='Published as a conference paper at ICLR 2024\\nr′\\nm+1andtm+1. To ensure proper calibration, we discard the obtained r′\\nm+1and initialize new\\nlearnable token embeddings t′\\nm+1. Note that even though the output r′\\nm+1is discarded, the updated\\ngradients can still be backpropagated to optimize the learnable tokens t′\\nmdue to the self-attention\\nmechanism. We repeat this operation until we reach the designated layer M′. During fine-tuning,\\nthese learnable token embeddings are optimized to refine the original textual space. More details\\nsee Appendix D.\\nInput\\n(a) Input\\nSimilarity map\\n(CLIP)Q-K attention mapMarker 1\\nSimilarity map\\n(AnomalyCLIP )\\n (b) Q-K attention\\nSimilarity map\\n(CLIP)Q-Q attention map\\nSimilarity map\\n(AnomalyCLIP )\\n (c) Q-Q attention\\nSimilarity map\\n(CLIP)K-K attention map\\nSimilarity map\\n(AnomalyCLIP )\\n (d) K-K attention\\nSimilarity map\\n(CLIP)V-V attention map\\nSimilarity map\\n(AnomalyCLIP ) (e) V-V attention\\nFigure 3: DPAM visualization.Refinement of the local visual space Since the visual\\nencoder of CLIP is originally pre-trained to align global\\nobject semantics, the contrastive loss used in CLIP makes\\nthe visual encoder produce a representative global em-\\nbedding for class recognition. Through the self-attention\\nmechanism, the attention map in the visual encoder fo-\\ncuses on the specific tokens highlighted within the red\\nrectangle in Fig 3b. Although these tokens may con-\\ntribute to global object recognition, they disrupt the lo-\\ncal visual semantics, which directly hinders the effective\\nlearning of the fine-grained abnormality in our object-\\nagnostic text prompts. We found empirically that a diag-\\nonally prominent attention map helps reduce the distur-\\nbance from other tokens, leading to improved local visual\\nsemantics. Therefore, we propose a mechanism called\\nDiagonally Prominent Attention Map to refine the local visual space, with the visual encoder kept\\nfrozen during training. To this end, we replace the original Q-Kattention in the visual encoder\\nwith diagonally prominent attention, such as Q-Q,K-K, and V-Vself-attention schemes. As\\ndemonstrated in Fig.3c, Fig.3d, and Fig. 3e, the refined DPAM attention maps are more diago-\\nnally prominent, resulting in substantially improved segmentation maps in both original CLIP and\\nour AnomalyCLIP. Compared to CLIP that is based on global features and manually defined text\\nprompts, the text prompts learned by AnomalyCLIP are more fine-grained, enabling substantially\\nmore accurate alignment between the normality/abnormality prompt embeddings and the local vi-\\nsual embeddings across four different self-attention schemes. This, in turn, allows AnomalyCLIP\\nto generate accurate SnandSafor the joint optimization in Eq. 2. Unless otherwise specified,\\nAnomalyCLIP utilizes V-Vself-attention due to its superior overall performance. The performance\\nof different self-attention mechanisms is analyzed in Sec. D. We also provide a detailed explanation\\nabout DPAM in Appendix C.\\nTraining and Inference During training, AnomalyCLIP minimizes the loss in Eq. 2 using an\\nauxiliary AD-related dataset. As for inference, given a test image xi, we use the similarity score\\nP(ga, fi)as the image-level anomaly score, with the anomaly score leaning toward one when the\\nanomaly textual embedding gais aligned with global visual embedding fi. For pixel-wise predic-\\ntions, we merge the segmentation SnandSaof all selected intermediate layers, followed by an\\ninterpolation and smoothing operation. Formally, our anomaly score map Map∈RHimage×Wimage\\nis computed as Map =Gσ(P\\nMk∈M(1\\n2(I−Up(Sn,Mk)) +1\\n2Up(Sa,Mk))), where Gσrepresents\\na Gaussian filter, and σcontrols smoothing.\\n4 E XPERIMENTS\\n4.1 E XPERIMENT SETUP\\nDatasets and Evaluation Metrics We conducted extensive experiments on 17 publicly available\\ndatasets, covering various industrial inspection scenarios and medical imaging domains (including\\nphotography, endoscopy, and radiology) to evaluate the performance of AnomalyCLIP. In industrial\\ninspection, we consider MVTec AD (Bergmann et al., 2019), VisA (Zou et al., 2022), MPDD (Jezek\\net al., 2021), BTAD (Mishra et al., 2021), SDD (Tabernik et al., 2020), DAGM (Wieler & Hahn,\\n2007), and DTD-Synthetic (Aota et al., 2023). In medical imaging, we consider skin cancer de-\\ntection dataset ISIC (Gutman et al., 2016), colon polyp detection datasets CVC-ClinicDB (Bernal\\net al., 2015), CVC-ColonDB (Tajbakhsh et al., 2015), Kvasir (Jha et al., 2020), and Endo (Hicks\\net al., 2021), thyroid nodule detection dataset TN3k (Gong et al., 2021), brain tumor detection\\n6'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 6}, page_content='Published as a conference paper at ICLR 2024\\nTable 1: ZSAD performance comparison on industrial domain. The best performance is highlighted\\nin red, and the second-best is highlighted in blue.†denotes results taken from original papers.\\nTask Category Datasets |C| CLIP CLIP-AC WinCLIP V AND CoOp AnomalyCLIP\\nImage-level\\n(AUROC, AP)Obj &texture MVTec AD 15 (74.1, 87.6) (71.5, 86.4) (91.8, 96.5)†(86.1, 93.5)†(88.8, 94.8) (91.5, 96.2)\\nObjVisA 12 (66.4, 71.5) (65.0, 70.1) (78.1, 81.2)†(78.0, 81.4)†(62.8, 68.1) (82.1, 85.4)\\nMPDD 6 (54.3, 65.4) (56.2, 66.0) (63.6, 69.9) (73.0, 80.2) (55.1, 64.2) (77.0, 82.0)\\nBTAD 3 (34.5, 52.5) (51.0, 62.1) (68.2, 70.9) (73.6, 68.6) (66.8, 77.4) (88.3, 87.3)\\nSDD 1 (65.7, 45.2) (65.2, 45.7) (84.3, 77.4) (79.8, 71.4) (74.9, 65.1) (84.7, 80.0)\\nTextureDAGM 10 (79.6, 59.0) (82.5, 63.7) (91.8, 79.5) (94.4, 83.8) (87.5, 74.6) (97.5, 92.3)\\nDTD-Synthetic 12 (71.6, 85.7) (66.8, 83.2) (93.2, 92.6) (86.4, 95.0) (-, -) (93.5, 97.0)\\nPixel-level\\n(AUROC, PRO)Obj &texture MVTec AD 15 (38.4, 11.3) (38.2, 11.6) (85.1, 64.6)†(87.6, 44.0)†(33.3, 6.7) (91.1, 81.4)\\nObjVisA 12 (46.6, 14.8) (47.8, 17.3) (79.6, 56.8)†(94.2, 86.8)†(24.2, 3.8) (95.5, 87.0)\\nMPDD 6 (62.1, 33.0) (58.7, 29.1) (76.4, 48.9) (94.1, 83.2) (15.4, 2.3) (96.5, 88.7)\\nBTAD 3 (30.6, 4.4) (32.8, 8.3) (72.7, 27.3) (60.8, 25.0) (28.6, 3.8) (94.2, 74.8)\\nSDD 1 (39.0, 8.9) (32.5, 5.8) (68.8, 24.2) (79.8, 65.1) (28.9, 7.1) (90.6, 67.8)\\nTextureDAGM 10 (28.2, 2.9) (32.7, 4.8) (87.6, 65.7) (82.4, 66.2) (17.5, 2.1) (95.6, 91.0)\\nDTD-Synthetic 12 (33.9, 12.5) (23.7, 5.5) (83.9, 57.8) (95.3, 86.9) (-, -) (97.9, 92.3)\\nTable 2: ZSAD performance comparison on medical domain. The best performance is highlighted\\nin red, and the second-best is highlighted in blue. Note that the image-level medical AD datasets do\\nnot contain segmentation ground truth, so the pixel-level medical AD datasets are different from the\\nimage-level datasets.\\nTask Category Datasets |C| CLIP CLIP-AC WinCLIP V AND CoOp AnomalyCLIP\\nImage-level\\n(AUROC, AP)BrainHeadCT 1 (56.5, 58.4) (60.0, 60.7) (81.8, 80.2) (89.1, 89.4) (78.4, 78.8) (93.4, 91.6)\\nBrainMRI 1 (73.9, 81.7) (80.6, 86.4) (86.6, 91.5) (89.3, 90.9) (61.3, 44.9) (90.3, 92.2)\\nBr35H 1 (78.4, 78.8) (82.7, 81.3) (80.5, 82.2) (93.1, 92.9) (86.0, 87.5) (94.6, 94.7)\\nChest COVID-19 1 (73.7, 42.4) (75.0, 45.9) (66.4, 42.9) (15.5, 8.5) (25.3, 9.2) (80.1, 58.7)\\nPixel-level\\n(AUROC, PRO)Skin ISIC 1 (33.1, 5.8) (36.0, 7.7) (83.3, 55.1) (89.4, 77.2) (51.7, 15.9) (89.7, 78.4)\\nColonCVC-ColonDB 1 (49.5, 15.8) (49.5, 11.5) (70.3,32.5) (78.4, 64.6) (40.5, 2.6) (81.9, 71.3)\\nCVC-ClinicDB 1 (47.5, 18.9) (48.5, 12.6) (51.2,13.8) (80.5, 60.7) (34.8, 2.4) (82.9, 67.8)\\nKvasir 1 (44.6, 17.7) (45.0, 16.8) (69.7, 24.5) (75.0, 36.2) (44.1, 3.5) (78.9, 45.6)\\nEndo 1 (45.2, 15.9) (46.6, 12.6) (68.2, 28.3) (81.9, 54.9) (40.6, 3.9) (84.1, 63.6)\\nThyroid TN3K 1 (42.3, 7.3) (35.6, 5.2) (70.7, 39.8) (73.6, 37.8) (34.0, 9.5) (81.5, 50.4)\\ndatasets HeadCT (Salehi et al., 2021), BrainMRI (Salehi et al., 2021), Br35H (Hamada., 2020), and\\nCOVID-19 detection dataset COVID-19 (Chowdhury et al., 2020; Rahman et al., 2021). The SOTA\\ncompeting methods include CLIP (Radford et al., 2021), CLIP-AC (Radford et al., 2021), Win-\\nCLIP (Jeong et al., 2023), V AND (Chen et al., 2023), and CoOp (Zhou et al., 2022b). We provide\\nmore details about the methods and data pre-processing in Appendix A. The anomaly detection per-\\nformance is evaluated using the Area Under the Receiver Operating Characteristic Curve (AUROC).\\nAdditionally, average precision (AP) for anomaly detection and AUPRO (Bergmann et al., 2020) for\\nanomaly segmentation are also used to provide more in-depth analysis of the performance.\\nImplementation details We use the publicly available CLIP model1(VIT-L/14@336px ) as our\\nbackbone. Model parameters of CLIP are all frozen. The length of learnable word embeddings Eis\\nset to 12. The learnable token embeddings are attached to the first 9 layers of the text encoder for\\nrefining the textual space, and their length in each layer is set to 4. We fine-tune AnomalyCLIP using\\nthe test data on MVTec AD and evaluate the ZSAD performance on other datasets. As for MVTec\\nAD, we fine-tune AomalyCLIP on the test data of VisA. We report dataset-level results, which are\\naveraged across their respective sub-datasets. All experiments are conducted in PyTorch-2.0.0 with\\na single NVIDIA RTX 3090 24GB GPU. More details can be found in Appendix A.\\n4.2 M AIN RESULTS\\nZSAD performance on diverse industrial inspection domains Table 1 shows the ZSAD results\\nof AnomalyCLIP with five competing methods over seven industrial defect datasets of very different\\nforeground objects, background, and/or anomaly types. AnomalyCLIP achieves superior ZSAD per-\\nformance across the datasets, substantially outperforming the other five methods in most datasets.\\nThe weak performance of CLIP and CLIP-AC can be attributed to CLIP’s original pre-training,\\nwhich focuses on aligning object semantics rather than anomaly semantics. By using manually\\ndefined text prompts, WinCLIP and V AND achieve better results. Alternatively, CoOp adopts learn-\\nable prompts to learn the global anomaly semantics. However, those prompts focus on the global\\n1https://github.com/mlfoundations/open clip\\n7'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 7}, page_content='Published as a conference paper at ICLR 2024\\nfeature and ignore the fine-grained local anomaly semantics, leading to their poor performance on\\nanomaly segmentation. To adapt CLIP to ZSAD, AnomalyCLIP learns object-agnostic text prompts\\nto focus on learning the generic abnormality/normality using global and local context optimization,\\nenabling the modeling of both global and local abnormality/normality. Our resulting prompts can\\nalso generalize to different datasets from various domains. To provide more intuitive results, we\\nvisualize the anomaly segmentation results of AnomalyCLIP, V AND, and WinCLIP across differ-\\nent datasets in Fig. 4. Compared to V AND and WinCLIP, AnomalyCLIP can perform much more\\naccurate segmentation for the defects from different industrial inspection domains.\\nGeneralization from defect datasets to diverse medical domain datasets To evaluate the gen-\\neralization ability of our model, we further examine the ZSAD performance of AnomalyCLIP on\\n10 medical image datasets of different organs across different imaging devices. Table 2 shows the\\nresults, where learning-based methods, including AnomalyCLIP, V AND and CoOp, are all tuned\\nusing MVTec AD data. It is remarkable that methods like AnomalyCLIP and V AND obtain promis-\\ning ZSAD performance on various medical image datasets, even though they are tuned using a\\ndefect detection dataset. Among all these methods, AnomalyCLIP is the best performer due to its\\nstrong generalization brought by object-agnostic prompt learning. As illustrated in Fig. 4, Anoma-\\nlyCLIP can accurately detect various types of anomalies in diverse medical images, such as skin\\ncancer regions in photography images, colon polyps in endoscopy images, thyroid nodules in ultra-\\nsound images, and brain tumors in MRI images, having substantially better performance in locating\\nthe abnormal lesion/tumor regions than the other two methods WinCLIP and V AND. This again\\ndemonstrates the superior ZSAD performance of AnomalyCLIP in datasets of highly diverse object\\nsemantics from medical imaging domains.\\nCan we obtain better ZSAD performance if fine-tuned using medical image data? Comparing\\nthe promising performance in industrial datasets, AnomalyCLIP presents a relatively low perfor-\\nmance in medical datasets. This is partly due to the impact of auxiliary data used in our prompt\\nlearning. So, then we examine whether the ZSAD performance on medical images can be improved\\nif the prompt learning is trained on an auxiliary medical dataset. One challenge is that there are\\nno available large 2D medical datasets that include both image-level and pixel-level annotations for\\nour training. To address this issue, we create such a dataset based on ColonDB (More details see\\nAppendix A), and then optimize the prompts in AnomalyCLIP and V AND using this dataset and\\nevaluate their performance on the medical image datasets. The results are presented in Table 3.\\nAnomalyCLIP and V AND largely improve their detection and segmentation performance compared\\nto that fine-tuned on MVTec AD, especially for the colon polyp-related datasets such as CVC-\\nClincDB, Kvasir, and Endo (note that these datasets are all from different domains compared to the\\nfine-tuning ColonDB dataset). AnomalyCLIP also exhibits performance improvement in detecting\\nbrain tumors in datasets such as HeadCT, BrainMRI, and Br35H. This is attributed to the visual\\nsimilarities between colon polyps and brain tumors. Conversely, the symptom of the colon polyp\\ndiffers significantly from that of diseased skin or chest, leading to performance degradation in ISIC\\nand COVID-19. Overall, compared to V AND, AnomalyCLIP performs consistently better across all\\ndatasets of anomaly detection and segmentation.\\nMedical domain\\nSkin Colon Thyroid Brain\\nOurs\\nVAND\\nWinCLIPGround\\nTruth\\nIndustrial domain\\nTile Screw Capsule Metal plate\\nFigure 4: Segmentation visualization.Table 3: ZSAD performance on medi-\\ncal images when fine-tuned by medical\\nimage datasets.\\nCategory Datasets V AND AnomalyCLIP\\nClassification\\nBrainHeadCT (89.1, 89.4) (93.5, 95.1)\\nBrainMRI (89.3, 90.9) (95.5, 97.2)\\nBr35H (93.1, 92.9) (97.9, 98.0)\\nChest COVID-19 (15.5, 8.5) (70.9, 33.7)\\nSegmentation\\nSkin ISIC (58.8, 31.2) (83.0, 63.8)\\nColonCVC-ClinicDB (89.4, 82.3) (92.4, 82.9)\\nKvasir (87.6, 39.3) (92.5, 61.5)\\nEndo (88.5, 81.9) (93.2, 84.8)\\nThyroid TN3K (60.5, 16.8) (79.2, 47.0)\\n8'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 8}, page_content='Published as a conference paper at ICLR 2024\\nTable 4: Module ablation.\\nModuleMVTec AD VisA\\nPixel-level Image-level Pixel-level Image-level\\nBase (46.8, 15.4) (66.3, 83.3) (47.9, 17.1) (54.4, 61.7)\\n+T1 (68.4, 47.4) (66.3, 83.3) (54.8, 32.7) (54.4, 61.7)\\n+T2 (89.5, 81.2) (90.8, 96.0) (95.0, 85.3) (81.7, 85.2)\\n+T3 (90.0, 81.1) (91.0, 96.1) (95.2, 86.0) (81.9, 85.2)\\n+T4 (91.1, 81.4) (91.5, 96.2) (95.5, 87.0) (82.1, 85.4)Table 5: Context optimization ablation.\\nLocal. Global.MVTec AD VisA\\nPixel-level Image-level Pixel-level Image-level\\n✗ ✗ (71.7, 57.7) (68.8, 85.8) (74.7, 62.1) (61.1, 69.1)\\n✗ ✓ (80.3, 77.8) (89.9, 95.4) (86.6, 78.1) (82.2, 84.9)\\n✓ ✗ (91.0, 80.4) (89.9, 96.0) (95.2, 86.5) (79.5, 83.2)\\n✓ ✓ (91.1, 81.4) (91.5, 96.2) (95.5, 87.0) (82.1, 85.4)\\nMVT ec AD\\nVisA\\nMPDD\\nBTAD SDDDAGMDTD\\n7580859095100Pixel-level AUROC\\nV-V attention Q-Q attention K-K attentionMVT ec AD\\nVisA\\nMPDD\\nBTAD SDDDAGMDTD\\n5060708090100AUPRO\\nMVT ec AD\\nVisA\\nMPDD\\nBTAD SDDDAGMDTD\\n7580859095100Image-level AUROC\\nMVT ec AD\\nVisA\\nMPDD\\nBTAD SDDDAGMDTD\\n7580859095100AP\\nFigure 6: DPAM component ablation.\\nMVT ec AD VisA MPDD BTAD SDD DAGM DTD\\nDataset01234Peformance gain\\n 0.8\\n 0.3 4.4\\n 0.3 0.8\\n 0.5\\n 0.1\\n 0.0 0.5 3.3\\n 0.3\\n 0.0 0.3\\n 0.0 0.6 0.5 0.8\\n 0.1 0.4 0.7\\n 0.0 1.0\\n 0.2 0.9 1.8\\n 0.9\\n 0.3 0.6Image-level metric              Pixel-level metric\\nImage-level AUROC\\nAPPixel-level AUROC\\nAUPRO\\nFigure 5: Performance gain of using object-agnostic\\nprompts compared to object-aware prompts.Object-agnostic vs. object-aware\\nprompt learning To study the ef-\\nfectiveness of object-agnostic prompt\\nlearning in AnomalyCLIP, we com-\\npare AnomalyCLIP with its vari-\\nant that uses an object-aware prompt\\ntemplate. The performance gain\\nof AnomalyCLIP to its object-aware\\nprompt learning variant is shown in\\nFig. 5, where positive values indicate\\nour object-agnostic prompt templates\\nare better than the object-aware one.\\nIt is clear that our object-agnostic\\nprompt learning performs much bet-\\nter than, or on par with, the object-\\naware version in both image-level and pixel-level anomaly detection. This indicates that having\\nobject-agnostic prompts helps better learn the generic abnormality and normality in images, as the\\nobject semantics are often not helpful, or can even become noisy features, for the ZSAD task.\\n4.3 A BLATION STUDY\\nModule ablation We first validate the effectiveness of different high-level modules of our Anoma-\\nlyCLIP, including DPAM ( T1), object-agnostic text prompts ( T2), added learnable tokens in text\\nencoders ( T3), and multi-layer visual encoder features ( T4). As shown in Table 4, each module\\ncontributes to the remarkable performance of AnomalyCLIP. DPAM improves the segmentation\\nperformance by enhancing local visual semantics ( T1). Object-agnostic text prompts focus on the\\nabnormality/normality within images instead of the object semantics, allowing AnomalyCLIP to de-\\ntect anomalies in diverse unseen objects. Therefore, introducing object-agnostic text prompts ( T2)\\nsignificantly improves AnomalyCLIP. Furthermore, text prompt tuning ( T3) also brings performance\\nimprovement via the refinement of original textual space. Finally, T4integrates multi-layer visual\\nsemantics to provide more visual details, which further promotes the performance of ZSAD.\\nContext optimization Next we examine key modules in detail. The object-agnostic prompt learn-\\ning is the most effective module, and it is driven by our glocal context optimization, so we consider\\ntwo different optimization terms, local and global losses, in Eq. 2. The results are shown in Table 5.\\nBoth global and local context optimization contribute to the superiority of AnomalyCLIP. Global\\ncontext optimization helps to capture global anomaly semantics, thus enabling more accurate image-\\nlevel detection. Compared to global context optimization, local context optimization incorporates\\nlocal anomaly semantics, which improves pixel-level performance and complements image-level\\nperformance. By synthesizing these two optimization strategies, AnomalyCLIP generally achieves\\nbetter performance than using them individually.\\n9'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 9}, page_content='Published as a conference paper at ICLR 2024\\nDPAM strategy ablation AnomalyCLIP uses V-Vself-attention by default. Here we study the\\neffectiveness of using two other DPAM strategies, including Q-QandK-Kself-attention, result-\\ning in two AnomalyCLIP variants, namely AnomalyCLIP qqand AnomalyCLIP kk. The compari-\\nson results are presented in Fig. 6. AnomalyCLIP qqachieves similar segmentation capabilities as\\nAnomalyCLIP but suffers from degradation in detecting image-level anomalies. Conversely, while\\nAnomalyCLIP kkperforms well in anomaly classification, its segmentation performance is less ef-\\nfective than AnomalyCLIP and AnomalyCLIP qq. The V-Vself-attention is generally recommended\\nin AnomalyCLIP. Detailed analysis of DPAM can be seen in Appendix C.\\n5 R ELATED WORK\\nZero-shot anomaly detection ZSAD relies on the model’s strong transferability to handle un-\\nseen anomalies (Aota et al., 2023). CLIP-AD (Liznerski et al., 2022) and ZOC (Esmaeilpour et al.,\\n2022) are early studies in utilizing CLIP for ZSAD, but they mainly focus on the anomaly classi-\\nfication task. ACR (Li et al., 2023a) requires tuning on target-domain-relevant auxiliary data for\\nZSAD on different target datasets, while AnomalyCLIP can be applied to different datasets after it\\nis trained on one general dataset. A very recent approach WinCLIP (Jeong et al., 2023) presents\\na seminal work that leverages CLIP for zero-shot classification and segmentation. It uses a large\\nnumber of hand-crafted text prompts and involves multiple forward passes of image patches for\\nanomaly segmentation. To tackle this inefficiency, V AND (Chen et al., 2023) introduces learnable\\nlinear projection techniques to enhance the modeling of local visual semantics. However, these ap-\\nproaches suffer from insufficiently generalized textual prompt embeddings, which degrades their\\nperformance in identifying anomalies associated with various unseen object semantics. Anomaly-\\nCLIP utilizes only two object-agnostic learnable text prompts to optimize the generic text prompts\\nof abnormality and normality, and it can obtain segmentation results with just a single forward pass.\\nAnomalyGPT (Gu et al., 2023) is a concurrent work in utilizing foundation models for AD, but it is\\ndesigned for unsupervised/few-shot AD with manually crafted prompts.\\nPrompt learning Rather than resorting to full network fine-tuning, prompt learning emerges as a\\nparameter-efficient alternative to achieve satisfactory results (Sun et al., 2022; Khattak et al., 2023;\\nKim et al., 2023; Zhou et al., 2022a). CoOp (Zhou et al., 2022b) introduces learnable text prompts\\nfor few-shot classification. On this basis, DenseCLIP (Rao et al., 2022) extends prompt learning\\nto dense prediction tasks with an extra image decoder. Instead, AnomalyCLIP proposes object-\\nagnostic prompt learning for anomaly detection, blocking out the potential adverse impact of the\\ndiverse object semantics on anomaly detection. Benefiting from the glocal context optimization,\\nAnomalyCLIP can capture local anomaly semantics such that we can simultaneously perform clas-\\nsification and segmentation tasks without an additional decoder network like Rao et al. (2022).\\n6 C ONCLUSION\\nIn this paper, we tackle a challenging yet significant area of anomaly detection, ZSAD, in which\\nthere is no available data in the target dataset for training. We propose AnomalyCLIP to improve the\\nweak generalization performance of CLIP for ZSAD. We introduce object-agnostic prompt learn-\\ning to learn generic abnormality/normality text prompts for generalized ZSAD on image datasets of\\ndiverse foreground objects. Further, to incorporate global and local anomaly semantics into Anoma-\\nlyCLIP, we devise a joint global and local context optimization to optimize the object-agnostic\\ntext prompts. Extensive experimental results on 17 public datasets demonstrate that AnomalyCLIP\\nachieves superior ZSAD performance.\\nACKNOWLEDGMENTS\\nThis work was supported by NSFC U1909207, NSFC 62088101 Autonomous Intelligent Un-\\nmanned Systems, and the Singapore Ministry of Education Academic Research Fund Tier 1 grant\\n(21SISSMU031).\\n10'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 10}, page_content='Published as a conference paper at ICLR 2024\\nREPRODUCIBILITY STATEMENT\\nTo ensure the reproducibility and completeness of this paper, we have included an Appendix consist-\\ning of five main sections. In Appendix A, we provide more implementation details of AnomalyCLIP,\\nas well as the reproduction of other baseline methods. Appendix B provides key statistics about the\\ndatasets used in our experiments and the implementation of the auxiliary medical dataset for prompt\\ntuning. Appendix D supplements the main paper with additional results and ablations. Further vi-\\nsualizations of similarity scores and maps are detailed in Appendix E. Additionally, the main paper\\npresents only the average performance in each dataset that contains a number of data subsets, for\\nwhich we present their fine-grained detection results, in Appendix F. Our code will be made publicly\\naccessible once the paper is accepted.\\nREFERENCES\\nToshimichi Aota, Lloyd Teh Tzer Tong, and Takayuki Okatani. Zero-shot versus many-shot: Un-\\nsupervised texture anomaly detection. In Proceedings of the IEEE/CVF Winter Conference on\\nApplications of Computer Vision , pp. 5564–5572, 2023.\\nPaul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad–a comprehen-\\nsive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition , pp. 9592–9600, 2019.\\nPaul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Uninformed students:\\nStudent-teacher anomaly detection with discriminative latent embeddings. In Proceedings of the\\nIEEE/CVF conference on computer vision and pattern recognition , pp. 4183–4192, 2020.\\nJorge Bernal, F Javier S ´anchez, Gloria Fern ´andez-Esparrach, Debora Gil, Cristina Rodr ´ıguez, and\\nFernando Vilari ˜no. Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation\\nvs. saliency maps from physicians. Computerized medical imaging and graphics , 43:99–111,\\n2015.\\nTri Cao, Jiawen Zhu, and Guansong Pang. Anomaly detection under distribution shift. arXiv preprint\\narXiv:2303.13845 , 2023.\\nXuhai Chen, Yue Han, and Jiangning Zhang. A zero-/few-shot anomaly classification and segmen-\\ntation method for cvpr 2023 vand workshop challenge tracks 1&2: 1st place on zero-shot ad and\\n4th place on few-shot ad. arXiv preprint arXiv:2305.17382 , 2023.\\nYuanhong Chen, Yu Tian, Guansong Pang, and Gustavo Carneiro. Deep one-class classification via\\ninterpolated gaussian descriptor. In Proceedings of the AAAI Conference on Artificial Intelligence ,\\nvolume 36, pp. 383–392, 2022.\\nMuhammad E. H. Chowdhury, Tawsifur Rahman, Amith Khandakar, Rashid Mazhar, Muham-\\nmad Abdul Kadir, Zaid Bin Mahbub, Khandakar Reajul Islam, Muhammad Salman Khan, Atif\\nIqbal, Nasser Al Emadi, Mamun Bin Ibne Reaz, and Mohammad Tariqul Islam. Can ai help\\nin screening viral and covid-19 pneumonia? IEEE Access , 8:132665–132676, 2020. doi:\\n10.1109/ACCESS.2020.3010287.\\nHanqiu Deng and Xingyu Li. Anomaly detection via reverse distillation from one-class embedding.\\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.\\n9737–9746, 2022.\\nChoubo Ding, Guansong Pang, and Chunhua Shen. Catching both gray and black swans: Open-set\\nsupervised anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pp. 7388–7398, 2022.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\\nimage is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 , 2020.\\n11'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 11}, page_content='Published as a conference paper at ICLR 2024\\nSepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei Shu. Zero-shot out-of-distribution detec-\\ntion based on the pre-trained model clip. In Proceedings of the AAAI conference on artificial\\nintelligence , volume 36, pp. 6568–6576, 2022.\\nTharindu Fernando, Harshala Gammulle, Simon Denman, Sridha Sridharan, and Clinton Fookes.\\nDeep learning for medical anomaly detection–a survey. ACM Computing Surveys (CSUR) , 54(7):\\n1–37, 2021.\\nHaifan Gong, Guanqi Chen, Ranran Wang, Xiang Xie, Mingzhi Mao, Yizhou Yu, Fei Chen, and\\nGuanbin Li. Multi-task learning for thyroid nodule segmentation with thyroid region prior. In\\n2021 IEEE 18th international symposium on biomedical imaging (ISBI) , pp. 257–261. IEEE,\\n2021.\\nZhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, and Jinqiao Wang. Anoma-\\nlygpt: Detecting industrial anomalies using large vision-language models, 2023.\\nDavid Gutman, Noel C. F. Codella, Emre Celebi, Brian Helba, Michael Marchetti, Nabin Mishra,\\nand Allan Halpern. Skin lesion analysis toward melanoma detection: A challenge at the inter-\\nnational symposium on biomedical imaging (isbi) 2016, hosted by the international skin imaging\\ncollaboration (isic), 2016.\\nA. Hamada. Br35h: Brain tumor detection 2020. Online. Available:\\nhttps://www.kaggle.com/datasets/ahmedhamada0/brain-tumor-detection , 2020.\\nSteven A Hicks, Debesh Jha, Vajira Thambawita, P ˚al Halvorsen, Hugo L Hammer, and Michael A\\nRiegler. The endotect 2020 challenge: evaluation and comparison of classification, segmentation\\nand inference time for endoscopy. In Pattern Recognition. ICPR International Workshops and\\nChallenges: Virtual Event, January 10-15, 2021, Proceedings, Part VIII , pp. 263–274. Springer,\\n2021.\\nChaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, Michael Spratling, and Yan-Feng Wang.\\nRegistration based few-shot anomaly detection. In European Conference on Computer Vision ,\\npp. 303–319. Springer, 2022.\\nJongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, and Onkar\\nDabeer. Winclip: Zero-/few-shot anomaly classification and segmentation. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 19606–19616, 2023.\\nStepan Jezek, Martin Jonak, Radim Burget, Pavel Dvorak, and Milos Skotak. Deep learning-based\\ndefect detection of metal parts: evaluating current methods in complex conditions. In 2021 13th\\nInternational congress on ultra modern telecommunications and control systems and workshops\\n(ICUMT) , pp. 66–71. IEEE, 2021.\\nDebesh Jha, Pia H Smedsrud, Michael A Riegler, P ˚al Halvorsen, Thomas de Lange, Dag Johansen,\\nand H ˚avard D Johansen. Kvasir-seg: A segmented polyp dataset. In MultiMedia Modeling: 26th\\nInternational Conference, MMM 2020, Daejeon, South Korea, January 5–8, 2020, Proceedings,\\nPart II 26 , pp. 451–462. Springer, 2020.\\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\\nSer-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision , pp. 709–727.\\nSpringer, 2022.\\nMuhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shah-\\nbaz Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pp. 19113–19122, 2023.\\nKwanyoung Kim, Yujin Oh, and Jong Chul Ye. Zegot: Zero-shot segmentation through optimal\\ntransport of text prompts. arXiv preprint arXiv:2301.12171 , 2023.\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\\narXiv:1412.6980 , 2014.\\n12'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 12}, page_content='Published as a conference paper at ICLR 2024\\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv\\npreprint arXiv:2304.02643 , 2023.\\nAodong Li, Chen Qiu, Marius Kloft, Padhraic Smyth, Maja Rudolph, and Stephan Mandt. Zero-shot\\nanomaly detection via batch normalization. In Thirty-seventh Conference on Neural Information\\nProcessing Systems , 2023a.\\nXiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, and Jiwei Li. Dice loss for data-\\nimbalanced nlp tasks. arXiv preprint arXiv:1911.02855 , 2019.\\nYi Li, Hualiang Wang, Yiqun Duan, and Xiaomeng Li. Clip surgery for better explainability with\\nenhancement in open-vocabulary tasks. arXiv preprint arXiv:2304.05653 , 2023b.\\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Focal loss for dense\\nobject detection. In Proceedings of the IEEE international conference on computer vision , pp.\\n2980–2988, 2017.\\nJie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett A Landman, Yixuan Yuan,\\nAlan Yuille, Yucheng Tang, and Zongwei Zhou. Clip-driven universal model for organ segmen-\\ntation and tumor detection. arXiv preprint arXiv:2301.00785 , 2023.\\nPhilipp Liznerski, Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Marius Kloft, and Klaus-\\nRobert M ¨uller. Explainable deep one-class classification. arXiv preprint arXiv:2007.01760 , 2020.\\nPhilipp Liznerski, Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Klaus-Robert M ¨uller, and\\nMarius Kloft. Exposing outlier exposure: What can be learned from few, one, and zero outlier\\nimages. arXiv preprint arXiv:2205.11474 , 2022.\\nPankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli, and Gian Luca Foresti. Vt-adl:\\nA vision transformer network for image anomaly detection and localization. In 2021 IEEE 30th\\nInternational Symposium on Industrial Electronics (ISIE) , pp. 01–06. IEEE, 2021.\\nShancong Mou, Xiaoyi Gu, Meng Cao, Haoping Bai, Ping Huang, Jiulong Shan, and Jianjun Shi.\\nRgi: robust gan-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly\\ndetection. In The Eleventh International Conference on Learning Representations , 2022.\\nGuansong Pang, Choubo Ding, Chunhua Shen, and Anton van den Hengel. Explainable deep few-\\nshot anomaly detection with deviation networks. arXiv preprint arXiv:2108.00462 , 2021a.\\nGuansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for\\nanomaly detection: A review. ACM computing surveys (CSUR) , 54(2):1–38, 2021b.\\nZiyuan Qin, Huahui Yi, Qicheng Lao, and Kang Li. Medical image understanding with pretrained\\nvision language models: A comprehensive study. arXiv preprint arXiv:2209.15517 , 2022.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\\nmodels from natural language supervision. In International conference on machine learning , pp.\\n8748–8763. PMLR, 2021.\\nTawsifur Rahman, Amith Khandakar, Yazan Qiblawey, Anas Tahir, Serkan Kiranyaz, Saad Bin Abul\\nKashem, Mohammad Tariqul Islam, Somaya Al Maadeed, Susu M Zughaier, Muhammad Salman\\nKhan, et al. Exploring the effect of image enhancement techniques on covid-19 detection using\\nchest x-ray images. Computers in biology and medicine , 132:104319, 2021.\\nYongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou,\\nand Jiwen Lu. Denseclip: Language-guided dense prediction with context-aware prompting.\\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.\\n18082–18091, 2022.\\nTal Reiss and Yedid Hoshen. Mean-shifted contrastive loss for anomaly detection. In Proceedings\\nof the AAAI Conference on Artificial Intelligence , volume 37, pp. 2155–2162, 2023.\\n13'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 13}, page_content='Published as a conference paper at ICLR 2024\\nKarsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch ¨olkopf, Thomas Brox, and Peter Gehler.\\nTowards total recall in industrial anomaly detection. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pp. 14318–14328, 2022.\\nLukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Gr ´egoire Montavon, Wojciech Samek,\\nMarius Kloft, Thomas G Dietterich, and Klaus-Robert M ¨uller. A unifying review of deep and\\nshallow anomaly detection. Proceedings of the IEEE , 109(5):756–795, 2021.\\nAneeshan Sain, Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Subhadeep Koley, Tao Xiang, and\\nYi-Zhe Song. Clip for all things zero-shot sketch-based image retrieval, fine-grained or not.\\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.\\n2765–2775, 2023.\\nMohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad H Rohban, and\\nHamid R Rabiee. Multiresolution knowledge distillation for anomaly detection. In Proceed-\\nings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 14902–14912,\\n2021.\\nXimeng Sun, Ping Hu, and Kate Saenko. Dualcoop: Fast adaptation to multi-label recognition\\nwith limited annotations. Advances in Neural Information Processing Systems , 35:30569–30582,\\n2022.\\nDomen Tabernik, Samo ˇSela, Jure Skvar ˇc, and Danijel Sko ˇcaj. Segmentation-based deep-learning\\napproach for surface-defect detection. Journal of Intelligent Manufacturing , 31(3):759–776,\\n2020.\\nNima Tajbakhsh, Suryakanth R Gurudu, and Jianming Liang. Automated polyp detection in\\ncolonoscopy videos using shape and context information. IEEE transactions on medical imaging ,\\n35(2):630–644, 2015.\\nYu Tian, Guansong Pang, Fengbei Liu, Yuanhong Chen, Seon Ho Shin, Johan W Verjans, Rajvin-\\nder Singh, and Gustavo Carneiro. Constrained contrastive distribution learning for unsupervised\\nanomaly detection and localisation in medical images. In Medical Image Computing and Com-\\nputer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France,\\nSeptember 27–October 1, 2021, Proceedings, Part V 24 , pp. 128–140. Springer, 2021.\\nYu Tian, Fengbei Liu, Guansong Pang, Yuanhong Chen, Yuyuan Liu, Johan W Verjans, Rajvinder\\nSingh, and Gustavo Carneiro. Self-supervised pseudo multi-class pre-training for unsupervised\\nanomaly detection and segmentation in medical images. Medical Image Analysis , pp. 102930,\\n2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\\ntion processing systems , 30, 2017.\\nMatthias Wieler and Tobias Hahn. Weakly supervised learning for industrial optical inspection. In\\nDAGM symposium in , volume 6, 2007.\\nSize Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and Chen Change Loy. Aligning bag of regions\\nfor open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pp. 15254–15264, 2023.\\nGuoyang Xie, Jingbao Wang, Jiaqi Liu, Feng Zheng, and Yaochu Jin. Pushing the limits of fewshot\\nanomaly detection in industry vision: Graphcore. arXiv preprint arXiv:2301.12082 , 2023.\\nZhiyuan You, Lei Cui, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, and Xinyi Le. A unified model for\\nmulti-class anomaly detection. Advances in Neural Information Processing Systems , 35:4571–\\n4584, 2022.\\nYiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li,\\nLuowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image\\npretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\\nnition , pp. 16793–16803, 2022.\\n14'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 14}, page_content='Published as a conference paper at ICLR 2024\\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for\\nvision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition , pp. 16816–16825, 2022a.\\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\\nlanguage models. International Journal of Computer Vision , 130(9):2337–2348, 2022b.\\nQihang Zhou, Shibo He, Haoyu Liu, Tao Chen, and Jiming Chen. Pull & push: Leveraging dif-\\nferential knowledge distillation for efficient unsupervised anomaly detection and localization.\\nIEEE Transactions on Circuits and Systems for Video Technology , 33(5):2176–2189, 2023. doi:\\n10.1109/TCSVT.2022.3218587.\\nYang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, and Onkar Dabeer. Spot-the-difference\\nself-supervised pre-training for anomaly detection and segmentation. In European Conference on\\nComputer Vision , pp. 392–408. Springer, 2022.\\nA I MPLEMENTATION DETAILS AND BASELINES\\nA.1 I MPLEMENTATION DETAILS\\nIn this paper, we use the publicly available CLIP model ( VIT-L/14@336px ) as our backbone.\\nModel parameters of CLIP are all frozen. The length of learnable text prompts Mis set to 12. These\\ntrainable text tokens are attached to the first 9 layers of the text encoder, and each text token has a\\nlength of 4. We fine-tune AnomalyCLIP on the test data on MVTec AD and test the performance\\nfor other datasets. As for MVTec AD, we fine-tune AomalyCLIP on test data on VisA. To provide\\nadequate visual details, we extract local visual embeddings vi\\nmfrom the 6-th, 12-th, 18-th, and 24-th\\nlayers of the visual encoder. Starting from the 6-th layer, we apply DPAM to the architecture of the\\nvisual encoder according to Sec. 3.3. Additionally, we set the balanced weight λto 1 in our loss\\nfunction. The input images are resized to a size of 518 with batch size 8, and we use the Adam\\noptimizer (Kingma & Ba, 2014) with a learning rate of 0.001 to update model parameters. During\\ntesting, we apply a Gaussian filter with σ= 4 to smooth the anomaly score map. The epoch is 15\\nfor all experiments, which are performed in PyTorch-2.0.0 with a single NVIDIA RTX 3090 24GB\\nGPU.\\nA.2 B ASELINES\\nTo demonstrate the superiority of Anomlay-CLIP, we compare AnomlayCLIP with broad SOTA\\nbaselines. Implementation and reproduction details are given as follows:\\n• CLIP (Radford et al., 2021). CLIP is a powerful zero-shot classification method.\\nTo perform the anomaly detection task, we use two classes of text prompt templates\\nA photo of a normal [cls] and A photo of an anomalous [cls] ,\\nwhere cls denotes the target class name. The anomaly score is computed according to\\nEq. 1. As for anomaly segmentation, we extend the above computation to local visual\\nembedding to derive the segmentation.\\n• CLIP-AC (Radford et al., 2021). Different from CLIP, CLIP-AC employs an ensemble of\\ntext prompt templates that are recommended for ImageNet dataset (Radford et al., 2021).\\nWe average the generated textual embeddings of normal and anomaly classes respectively,\\nand compute the probability and segmentation in the same way as CLIP.\\n• WinCLIP (Jeong et al., 2023). WinCLIP is a SOTA ZSAD method. They design a large\\nset of hand-crafted text prompt templates specific to anomaly detection and use a window\\nscaling strategy to obtain anomaly segmentation. All parameters are kept the same as in\\ntheir paper.\\n• V AND (Chen et al., 2023). V AND is an improved version of WinCLIP. They first adjust\\nthe text prompt templates and then introduce learnable linear projections to improve local\\nvisual semantics to derive more accurate segmentation. All parameters are kept the same\\nas in their paper.\\n15'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 15}, page_content='Published as a conference paper at ICLR 2024\\nTable 6: Key statistics on the datasets used.\\nDataset Category Modalities |C|Normal and\\nanomalous samplesUsage\\nMVTec AD Obj &texture Photography 15 (467, 1258) Industrial defect detection\\nVisA\\nObjPhotography 12 (962, 1200) Industrial defect detection\\nMPDD Photography 6 (176, 282) Industrial defect detection\\nBTAD Photography 3 (451, 290) Industrial defect detection\\nSDD Photography 1 (181, 74) Industrial defect detection\\nDAGMTexturePhotography 10 (6996, 1054) Industrial defect detection\\nDTD-Synthetic Photography 12 (357, 947) Industrial defect detection\\nISIC Skin Photography 1 (0, 379) Skin cancer detection\\nCVC-ClinicDB Endoscopy 1 (0, 612) Colon polyp detection\\nCVC-ColonDB Endoscopy 1 (0, 380) Colon polyp detection\\nKvasir Endoscopy 1 (0, 1000) Colon polyp detection\\nEndo Endoscopy 1 (0, 200) Colon polyp detection\\nTN3K ThyroidRadiology\\n(Utralsound)1 (0, 614) Thyroid nodule detection\\nHeadCT\\nBrainRadiology\\n(CT)1 (100, 100) Brain tumor detection\\nBrainMRIRadiology\\n(MRI)1 (98, 155) Brain tumor detection\\nBr35HRadiology\\n(MRI)1 (1500, 1500) Brain tumor detection\\nCOVID-19 ChestRadiology\\n(X-ray)1 (1341, 219) COVID-19 detection\\n• CoOp (Zhou et al., 2022b). CoOp is a representative method for prompt learning. To\\nadapt CoOp to ZSAD, we replace its learnable text prompt templates [V1][V2]...[VN][cls]\\nwith normality and abnormality text prompt templates, where Viis the learnable word em-\\nbeddings. The normality text prompt template is defined as [V1][V2]...[VN][normal ][cls],\\nand the abnormality one is defined as [V1][V2]...[VN][anomalous ][cls]. Anomaly proba-\\nbilities and segmentation are obtained in the same way as for AnomalyCLIP. All parameters\\nare kept the same as in their paper.\\nB D ATASET\\nMore dataset details In this paper, we conduct extensive experiments on 17 public datasets span-\\nning two domains and three modalities to validate the effectiveness of our methods. Since we just\\nuse the test data of Datasets, we present the relevant information of their test sets in Table 6. We\\napply the default normalization of OpenCLIP to all datasets. After normalization, we resize the\\nimages to a resolution of (518, 518) to obtain an appropriate visual feature map resolution. It should\\nbe noted that the original image size of SDD has a width of 500 and a height ranging from 1,240 to\\n1,270. Before processing, we vertically divide the original 500 × 1,250 image into two images and\\nassign pixel-wise annotations to each image.\\nFine-tuning medical dataset We cannot find publicly available 2D medical AD datasets that in-\\nclude both category labels and segmentation ground truths simultaneously. To fill the blank, in this\\npaper, we create such a medical dataset by combining two existing 2D medical datasets. Particularly,\\nwe use the colon polyp detection dataset ColonDB (Tajbakhsh et al., 2015) to provide pixel-level an-\\nnotations. Meanwhile, considering the normal samples in the same domain, we choose the test split\\nof Endo classification dataset (Hicks et al., 2021) to combine with ColonDB. As a result, the new\\nmedical dataset contains 163 normal samples and 380 anomaly samples, supporting both anomaly\\nclassification and segmentation tasks.\\n16'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 16}, page_content='Published as a conference paper at ICLR 2024\\nC D ETAILED ANALYSIS OF DPAM\\nSince the visual encoder of CLIP is originally pre-trained to align global object semantics, such as cat\\nand dog, the contrastive loss used in CLIP makes the visual encoder produce a representative global\\nembedding for recognizing semantic classes. Through the self-attention mechanism, the attention\\nmap in the visual encoder focuses on the specific tokens highlighted within the red rectangle in\\nFig. 3b. Although these tokens may contribute to global object recognition, they disrupt the local\\nvisual semantics, which directly hinders the effective learning of the fine-grained abnormality in our\\nobject-agnostic text prompts. For segmentation purposes, it’s crucial for the visual feature map to\\nemphasize the surrounding context to capture more local visual semantics.\\nFormally, let aijbe an attention score in the attention score matrix, where i, j∈[1, h×w], then the\\ni-th output of Q-Kattention can be written as:\\nAttention (Q, K, V )i=softmax\\x12qiK⊤\\n√\\nD\\x13\\nV=nP\\nj=1aijvj\\nnP\\nj=1aij, a ij=eqik⊤\\nj√\\nD.\\nNote that vectors (i.e., qi,ki,vi) are represented as row vectors. Attention (Q, K, V )ican be re-\\ngarded as the weighted average of vjusing aijas the weight. Assuming that the original attention\\nmap focuses on the specific tokens at index m, it is clear that qionly produces the large attention\\nscore with kmin allkj. Therefore, aimis the largest score among other aijsoAttention (Q, K, V )i\\nis dominated by vm, which causes the local visual embedding at index ito be disturbed by the lo-\\ncal visual embedding at index m. In Figure 3(b), the attention score map presents vertical activa-\\ntion and suggests that every qiproduces a large attention score with km. In such a case, several\\nAttention (Q, K, V )iis dominated by vmand results in weak anomaly segmentation in Figure 3(b)\\neven though vmmay be important for original class recognition. Some prior studies (Rao et al.,\\n2022; Gu et al., 2023) use an additional decoder to recover the local visual semantics. In this paper,\\nwe directly use local visual embeddings for segmentation and point out that an ideal attention map\\nfor local visual semantics should exhibit a more pronounced diagonal pattern. For this purpose,\\nDPAM is proposed to replace the original Q-Kattention with analogous components, including\\nQ-Q,K-K, andV-Vself-attention. Therefore, aijis changed into:\\naqq\\nij=eqiq⊤\\nj√\\nD, akk\\nij=ekik⊤\\nj√\\nD, avv\\nij=eviv⊤\\nj√\\nD.\\nThis modification ensures that qi,ki, and vihold significant weight in forming\\nAttention (Q, Q, V )i,Attention (K, K, V )i, and Attention (V, V, V )i, thereby preserving\\nlocal visual semantics. As a result, the produced attention maps exhibit a more diagonal prominence\\ncompared to the original Q-K attention, leading to improved performance in anomaly segmen-\\ntation, as shown in Fig.3c, Fig.3d, and Fig. 3e. However, since QandKconsist of the original\\nattention map, other important tokens at index nfor class recognition within themselves may also\\nproduce relatively large scores ( ain) (e.g., qihas strong relevance with qnbesides qi) to disturb\\nAttention (Q, Q, V )iandAttention (K, K, V )iFig.3c and Fig.3d. In contrast to Q-QandK-K,\\nV-Vdoes not participate in computing the original attention map, reducing the unexpected bias\\nto different tokens in Vfor the purpose of anomaly segmentation. Therefore, vidoes not produce\\na large weight ( aij) with vjand generates a larger weight ( aii) to form Attention (V, V, V )i,\\npreserving more information of viand experiencing diagonally prominent attention map (minimal\\ndisturbance), as depicted in Fig. 3e. This is the reason why V-Vachieves the best results.\\nD A DDITIONAL RESULTS AND ABLATIONS\\nComparison with SOTA full-shot methods In this section, we are interested in the performance\\ngap between AnomalyCLIP and the recently published SOTA full-shot methods, such as Patch-\\nCore (Roth et al., 2022) and RD4AD (Deng & Li, 2022). Since some datasets do not provide normal\\ntraining data, we conduct experiments on six public datasets. AnomalyCLIP achieves comparable\\nanomaly detection and segmentation performance compared to PatchCore and RD4AD, and it even\\noutperforms them in some datasets. This illustrates that the generic prompt embeddings empower\\nAnomalyCLIP to effectively capture the normality and abnormality so that AnomalyCLIP can sur-\\npass the performance boundary decided by the training data.\\n17'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 17}, page_content='Published as a conference paper at ICLR 2024\\nTable 7: Comparison of ZSAD performance between AnomalyCLIP and SOTA full-shot methods.\\nThe best performance is highlighted in red, and the second-best is highlighted in blue.\\nTask Category Datasets |C| AnomalyCLIP PatchCore RD4AD\\nImage-level\\n(AUROC, AP)Obj &texture MVTec AD 15 (91.5, 96.2) (99.0, 99.7) (98.7, 99.4)\\nObjVisA 12 (82.1, 85.4) (94.6, 95.9) (95.3, 95.7)\\nMPDD 6 (77.0, 82.0) (94.1, 96.3) (91.6, 93.8)\\nBTAD 3 (88.3, 87.3) (93.2, 98.6) (93.8, 96.8)\\nSDD 1 (84.7, 80.0) (64.9, 48.3) (86.8, 81.3)\\nTexture DAGM 10 (97.5, 92.3) (92.7, 81.3) (92.9, 79.1)\\nPixel-level\\n(AUROC, PRO)Obj &texture MVTec AD 15 (91.1, 81.4) (98.1, 92.8) (97.8, 93.6)\\nObjVisA 12 (95.5, 87.0) (98.5, 92.2) (98.4, 91.2)\\nMPDD 6 (96.5, 88.7) (98.8, 94.9) (98.4, 95.2)\\nBTAD 3 (94.2, 74.8) (97.4, 74.4) (97.5, 75.1)\\nSDD 1 (90.6, 67.8) (87.9, 46.3) (92.2, 72.0)\\nTexture DAGM 10 (95.6, 91.0) (95.9, 87.9) (96.8, 91.9)\\n10 12 14 1680.082.585.087.5\\nMVT ec\\nVisA\\n10 12 14 16859095100\\nMVT ec\\nVisA\\nAblation on the length of learnable text prompt E\\n(a)\\n5 7 9 1180.082.585.087.5\\nMVT ec\\nVisA\\n5 7 9 11859095100\\nMVT ec\\nVisA\\nAblation on the depth of learnable text prompt M (b)\\n2 4 6 880.082.585.087.5\\nMVT ec\\nVisA\\n2 4 6 880859095\\nMVT ec\\nVisA\\nAblation on the length of learnable token embedding L\\n(c)\\n0 1 2 3 4758085\\nMVT ec\\nVisA\\n0 1 2 3 480859095\\nMVT ec\\nVisA\\nAblation on used layer of visial encoder N (d)\\nFigure 7: Hyparameter analysis. (a) Eablation. (b) Mablation. (c) Lablation (d) Nablation.\\nPixel/image-level (AUPRO, AP) performances are shown on the left and right sides of each subplot,\\nrespectively.\\nRefinement of the textual space A representative embedding is not only decided by the well-\\ndesigned text prompt, it also depends on the appropriate textual space. During fine-tuning, randomly\\ninitialized learnable token embeddings are introduced in the text encoder to refine the textual space\\nfor the adaption to AD. To control the degree of refining the textual space, we choose to insert the\\nlearnable token embeddings into the text encoder from its bottom to the top layer. In particular, the\\ntrainable and original tokens are denoted as t′\\nmandtm, respectively, where mrepresents the layer of\\nthe text encoder. To integrate the original textual representations, for the layer m, we concatenate t′\\nm\\nandtmalong the dimension of the channel and then forward them into Tmto get r′\\nm+1andtm+1.\\nDue to the self-attention mechanism, the output of tm+1contains the information of t′\\nm. In order\\nto provide adequate calibration, we discard the obtained r′\\nm+1and initialize new learnable token\\nembeddings t′\\nm+1. Through this operation, t′\\nm+1further refines textual representations of the layer\\nm+ 1. We repeat this operation until we reach the designated layer M′. This procedure is given by:\\n[r′\\nm+1, tm+1] =Tm([t′\\nm, tm])\\n[r′\\nm+2, tm+2] =Tm+1([t′\\nm+1, tm+1]) (3)\\n. . .\\ntM′+1=TM′(tM′),\\nwhere the operator [·,·]represents the concatenation along the channel.\\nHyparameter analysis We study the length of learnable text prompts E, depth of learnable token\\nembeddings M, length of learnable token embeddings M, and number of used layers in visual\\nencoder N. As shown in Fig. 7b, we observe that the detection and segmentation performance\\ninitially improves with an increase in the value of E. However, within the range of lengths from 12 to\\n16, we notice a decline in performance, which suggests that excessively long learnable text prompts\\ncould involve redundant information. Therefore, an appropriate value for E, such as E= 12 , is\\n18'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 18}, page_content='Published as a conference paper at ICLR 2024\\nTable 8: Ablation on the robustness of the abnormality-related token in our prompt template on\\nindustrial defect datasets.\\nTask Category Datasets damaged anomalous flawed defective blemished\\nImage-level\\n(AUROC, AP)Obj &texture MVTec AD (91.5, 96.2) (91.4, 96.2) (91.3, 96.2) (91.4, 96.2) (91.5, 96.2)\\nObjVisA (82.1, 85.4) (80.7, 84.5) (80.7, 84.5) (80.9, 84.6) (80.7, 84.5)\\nMPDD (77.0, 82.0) (78.0, 83.9) (77.9, 83.6) (77.8, 83.5) (78.6. 84.1)\\nBTAD (88.3, 87.3) (84.8, 86.7) (85.2, 87.4) (84.8, 86.2) (85.9, 67.1)\\nSDD (84.7, 80.0) (82.3, 76.3) (82.6, 76.8) (82.8, 77.2) (82.7, 77.0)\\nTextureDAGM (97.5, 92.3) (97.7, 92.6) (97.5, 92.4) (97.5, 92.3) (97.5, 92.4)\\nDTD-Synthetic (93.5, 97.0) (93.3, 96.9) (93.2, 96.9) (93.4, 97.0) (93.5, 97.0)\\nPixel-level\\n(AUROC, PRO)Obj &texture MVTec AD (91.1, 81.4) (91.0, 81.4) (90.7, 81.4) (91.0, 81.7) (90.9, 81.2)\\nObjVisA (95.5, 87.0) (95.5, 86.5) (95.5, 86.5) (95.5, 86.2) (95.6, 86.5)\\nMPDD (96.5, 88.7) (96.6, 88.7) (96.7, 89.0) (96.7, 89.2) (96.6, 88.8)\\nBTAD (94.2, 74.8) (94.3, 74.3) (94.4, 75.1) (94.3, 75.2) (94.3, 73.7)\\nSDD (90.6, 67.8) (89.6, 66.8) (89.5, 66.5) (89.5, 64.8) (89.6, 64.6)\\nTextureDAGM (95.6, 91.0) (95.6, 91.2) (95.6, 91.3) (95.5, 90.9) (95.6, 90.9)\\nDTD-Synthetic (97.9, 92.3) (97.9, 92.3) (97.9, 92.1) (97.9, 92.5) (97.9, 92.2)\\nMVT ec AD VisA91.5\\n82.189.1\\n82.0Image-level AUROC AUROC\\nAnomalyCLIP\\nAnomalyCLIPre\\nMVT ec AD VisA96.2\\n85.494.5\\n85.0AP\\nAnomalyCLIP\\nAnomalyCLIPre\\nMVT ec AD VisA91.195.5\\n90.595.0Pixel-level AUROC\\nAnomalyCLIP\\nAnomalyCLIPre\\nMVT ec AD VisA81.487.0\\n80.686.6AUPRO\\nAnomalyCLIP\\nAnomalyCLIPre\\nFigure 8: Object ablation.\\nbeneficial to accurate learning of object-agnostic text prompts. Besides, we also investigate the depth\\nof the attached learnable token embeddings in Fig. 7b. The degree of refining of the initial text space\\nbecomes more pronounced as the depth increases, enabling more discriminative textual embeddings\\nfor normal and anomaly. However, the performance drops when the refinement is excessive and\\nimpairs the generalization of AnomlayCLIP, as seen in the case when Mequals 9. After selecting\\nthe depth, we proceed to investigate the influence of the length of learnable token embeddings. As\\nillustrated in Fig. 7c, we find that the length of token embeddings also involves a similar tradeoff\\nbetween the model generalization and calibration of textual space in Fig. 7d. AnomalyCLIP achieves\\nthe overall performance gain when we provide the most local visual semantics ( N= 4).\\nPrompt template ablation Here, we study the robustness of AnomalyCLIP to prior anomaly\\nsemantics in the object-agnostic text prompt template. We replace damaged in the object-agnostic\\ntext prompt with other words having similar anomaly semantics, such as anomalous ,flawed ,\\ndefective ,blemished . The results are presented in Table 8 and Table 9. The steady results\\nindicate that AnomalyCLIP is not sensitive to the prior anomaly semantics introduced by the object-\\nagnostic text prompt template.\\nObject ablation To investigate what the object-agnostic text prompts have learned, we replace\\nobject in object-agnostic text prompts with specific target [cls] , resulting in AnomalyCLIP re.\\nIn Fig. 8, AnomalyCLIP restill performs well in ZSAD, even as we block out the object seman-\\ntics during fine-tuning. This suggests that the knowledge learned by object-agnostic text prompts\\nis the underlying anomaly patterns, allowing them to provide discriminative textual embeddings\\neven when specific object semantics are incorporated. Furthermore, compared to AnomalyCLIP,\\nAnomalyCLIP reshows a performance decay, which can be attributed to the inclusion of redun-\\ndant/noisy object semantics. These results once again demonstrate the generalization ability of\\nobject-agnostic prompt learning.\\n19'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 19}, page_content='Published as a conference paper at ICLR 2024\\nTable 9: Ablation on the robustness of the abnormality-related token in our prompt template on\\nmedical image datasets.\\nTask Category Datasets damaged anomalous flawed defective blemished\\nImage-level\\n(AUROC, AP)BrainHeadCT (93.4, 91.6) (93.1, 90.6) (93.3, 90.8) (93.5, 91.0) (93.8, 91.5)\\nBrainMRI (90.3, 92.2) (87.8, 90.4) (87.7, 90.0) (88.3, 90.5) (88.6, 90.7)\\nBr35H (94.6, 94.7) (93.1, 93.0) (92.9, 92.8) (93.1, 93.0) (93.2, 93.1)\\nChest COVID-19 (80.1, 58.7) (80.0, 58.5) (80.2, 58.8) (80.6, 59.0) (82.1, 61.4)\\nPixel-level\\n(AUROC, PRO)Skin ISIC (89.7, 78.4) (90.1, 80.1) (90.1, 80.1) (90.4, 81.0) (90.2, 80.6)\\nColonCVC-ColonDB (81.9, 71.3) (82.2, 71.5) (82.3, 71.6) (82.1, 71.1) (82.2, 71.5)\\nCVC-ClinicDB (82.9, 67.8) (83.0, 68.1) (83.1, 68.4) (82.9, 67.9) (83.1, 68.2)\\nKvasir (78.9, 45.6) (79.4, 45.1) (79.4, 45.2) (79.3, 44.9) (79.5, 45.8)\\nEndo (84.1, 63.6) (84.3, 63.5) (84.2, 63.5) (84.2, 62.9) (84.3, 63.4)\\nThyroid TN3K (81.5, 50.4) (81.5, 51.7) (81.3, 50.9) (81.3, 50.3) (81.6, 51.1)\\nTable 10: Fine-grained data-subset-wise performance comparison (AUROC) for anomaly segmen-\\ntation on MVTec AD.\\nObject name CLIP CLIP-AC WinCLIP V AND CoOp AnomalyCLIP\\nCarpet 11.5 10.7 95.4 98.4 6.7 98.8\\nBottle 17.5 23.3 89.5 83.4 23.1 90.4\\nHazelnut 25.2 34.0 94.3 96.1 30.2 97.1\\nLeather 9.9 5.6 96.7 99.1 11.7 98.6\\nCable 37.4 37.5 77.0 72.3 49.7 78.9\\nCapsule 50.9 49.1 86.9 92.0 35.5 95.8\\nGrid 8.7 11.9 82.2 95.8 7.8 97.3\\nPill 55.8 60.8 80.0 76.2 46.5 92\\nTransistor 51.1 48.5 74.7 62.4 50.1 71\\nMetal nut 43.9 53.6 61.0 65.4 49.3 74.4\\nScrew 80.1 76.4 89.6 97.8 17.0 97.5\\nToothbrush 36.3 35.0 86.9 95.8 64.9 91.9\\nZipper 51.5 44.7 91.6 91.1 33.4 91.4\\nTile 49.9 39.1 77.6 92.7 41.7 94.6\\nWood 45.7 42.4 93.4 95.8 31.4 96.5\\nMean 38.4 38.2 85.1 87.6 33.3 91.1\\nE V ISUALIZATION\\nSimilarity score between textual and visual embeddings. We present visualizations of the sim-\\nilarity scores generated by both CLIP and AnomalyCLIP. These visualizations aim to provide an\\nintuitive illustration of the effective adaptation made by AnomalyCLIP in comparison to CLIP. As\\nshown in Fig. 9 and Fig. 10, we present the similarity score of CLIP on MVTec AD and VisA. The\\nnormal and anomaly scores are severely overlapped. Further, the range of scores is centered at 0.5.\\nThese show that the textual and visual space of CLIP originally aligned for object semantics are not\\ndesired for ZSAD. Also, we visualize the similarity scores of AnomalyCLIP in Fig. 11 and Fig. 12.\\nCompared to CLIP, there is a significant overlap between the scores assigned to normal and anomaly\\ninstances, and at the same time, the score range is considerably wider. These results indicate that\\nAnomalyCLIP achieves a significant improvement in adapting CLIP to ZSAD.\\nAnomaly score map for different datasets. In addition to the similarity score for anomaly clas-\\nsification, we also visualize the anomaly score maps to present the strong anomaly segmentation\\nability of AnomalyCLIP. Specifically, we visualize the industrial object class: hazelnut, pill, and\\nscrew from MVTec AD; candle, chewinggum, capsule, cashew, pcb, and pip fryum from Visa;\\nbracket, metal plate, and tube from MPDD. We also visualize the industrial texture: grid, leather,\\ncarpet, tile, wood, and zipper. In addition, we visualize the segmentation in medical domain across\\nphotography, endoscopy, and radiology images: skin cancer detection from ISIC; thyroid nodule\\ndetection from TN3K; colon polyp detection from Kvasir; brain tumor detection from Br35H.\\nF F INE-GRAINED ZSAD PERFORMANCE\\nIn this section, we present the fine-grained data subset-level ZSAD performance in details.\\n20'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 20}, page_content='Published as a conference paper at ICLR 2024\\nTable 11: Fine-grained data-subset-wise performance comparison (PRO) for anomaly segmentation\\non MVTec AD.\\nObject name CLIP CLIP-AC WinCLIP V AND CoOp AnomalyCLIP\\nCarpet 2.9 1.9 84.1 48.5 0.5 90.1\\nBottle 1.4 4.9 76.4 45.6 4.5 80.9\\nHazelnut 2.8 9.4 81.6 70.3 4.7 92.4\\nLeather 0.2 0.0 91.1 72.4 1.8 92.2\\nCable 7.3 6.9 42.9 25.7 12.2 64.4\\nCapsule 13.2 14.9 62.1 51.3 5.7 87.2\\nGrid 0.9 2.4 57.0 31.6 1.0 75.6\\nPill 6.0 8.2 65.0 65.4 3.2 88.2\\nTransistor 15.3 11.2 43.4 21.3 9.3 58.1\\nMetal nut 2.9 10.3 31.8 38.4 7.0 71.0\\nScrew 57.8 56.2 68.5 67.1 6.4 88.0\\nToothbrush 5.8 5.2 67.7 54.5 16.6 88.5\\nZipper 17.7 15.2 71.7 10.7 11.6 65.3\\nTile 21.5 16.3 51.2 26.7 10.1 87.6\\nWood 13.7 10.3 74.1 31.1 5.1 91.2\\nMean 11.3 11.6 64.6 44.0 6.7 81.4\\nTable 12: Fine-grained data-subset-wise performance comparison (AUROC) for anomaly classifica-\\ntion on MVTec AD.\\nObject name CLIP CLIP-AC WinCLIP V AND CoOp AnomalyCLIP\\nCarpet 96 93.1 100.0 99.5 99.9 100.0\\nBottle 45.9 46.1 99.2 92.0 87.7 89.3\\nHazelnut 88.7 91.1 93.9 89.6 93.5 97.2\\nLeather 99.4 99.5 100.0 99.7 99.9 99.8\\nCable 58.1 46.6 86.5 88.4 56.7 69.8\\nCapsule 71.4 68.8 72.9 79.9 81.1 89.9\\nGrid 72.5 63.7 98.8 86.3 94.7 97.0\\nPill 73.6 73.8 79.1 80.5 78.6 81.8\\nTransistor 48.8 51.2 88.0 80.8 92.2 92.8\\nMetal nut 62.8 63.4 97.1 68.4 85.3 93.6\\nScrew 78.2 66.7 83.3 84.9 88.9 81.1\\nToothbrush 73.3 89.2 88.0 53.8 77.5 84.7\\nZipper 60.1 36.1 91.5 89.6 98.8 98.5\\nTile 88.5 89.0 100.0 99.9 99.7 100.0\\nWood 94 94.9 99.4 99.0 97.7 96.8\\nMean 74.1 71.5 91.8 86.1 88.8 91.5\\nTable 13: Fine-grained data-subset-wise performance comparison (AP) on for anomaly classification\\nMVTec AD.\\nObject name CLIP CLIP-AC WinCLIP V AND CoOp AnomalyCLIP\\nCarpet 98.8 97.8 100.0 99.8 100.0 100.0\\nBottle 78.9 79.8 99.8 97.7 96.4 97.0\\nHazelnut 94.6 95.9 96.9 94.8 96.7 98.6\\nLeather 99.8 99.8 100.0 99.9 100.0 99.9\\nCable 70.8 64.3 91.2 93.1 69.4 81.4\\nCapsule 92.1 90.9 91.5 95.5 95.7 97.9\\nGrid 87.1 83.9 99.6 94.9 98.1 99.1\\nPill 93.4 93.6 95.7 96.0 94.2 95.4\\nTransistor 48.1 49.9 87.1 77.5 90.2 90.6\\nMetal nut 87.7 89.2 99.3 91.9 96.3 98.5\\nScrew 91.4 86.6 93.1 93.6 96.2 92.5\\nToothbrush 90.7 96.0 95.6 71.5 90.4 93.7\\nZipper 87.4 73.9 97.5 97.1 99.7 99.6\\nTile 95.9 96.2 100.0 100.0 99.9 100.0\\nWood 97.9 98.3 99.8 99.7 99.4 99.2\\nMean 87.6 86.4 96.5 93.5 94.8 96.2\\nTable 14: Fine-grained data-subset-wise performance comparison (AUROC) for anomaly segmen-\\ntation on VisA.\\nObject name CLIP CLIP-AC WinCLIP V AND CoOp AnomalyCLIP\\nCandle 33.6 50.0 88.9 97.8 16.3 98.8\\nCapsules 56.8 61.5 81.6 97.5 47.5 95.0\\nCashew 64.5 62.5 84.7 86.0 32.5 93.8\\nChewinggum 43.0 56.5 93.3 99.5 3.4 99.3\\nFryum 45.6 62.7 88.5 92.0 21.7 94.6\\nMacaroni1 20.3 22.9 70.9 98.8 36.8 98.3\\nMacaroni2 37.7 28.8 59.3 97.8 27.5 97.6\\nPcb1 57.8 51.6 61.2 92.7 19.8 94.1\\nPcb2 34.7 38.4 71.6 89.7 22.9 92.4\\nPcb3 54.6 44.6 85.3 88.4 18.0 88.4\\nPcb4 52.1 49.9 94.4 94.6 14.0 95.7\\nPipe fryum 58.7 44.7 75.4 96.0 29.2 98.2\\nMean 46.6 47.8 79.6 94.2 24.2 95.5\\n21'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 21}, page_content='Published as a conference paper at ICLR 2024\\nTable 15: Fine-grained data-subset-wise performance comparison (PRO) for anomaly segmentation\\non VisA.\\nObject name CLIP CLIP-AC WinCLIP V AND CoOp AnomalyCLIP\\nCandle 3.6 6.0 83.5 92.5 1.1 96.2\\nCapsules 15.8 22.4 35.3 86.7 18.4 78.5\\nCashew 9.6 10.9 76.4 91.7 1.7 91.6\\nChewinggum 17.8 30.2 70.4 87.3 0.1 91.2\\nFryum 12.1 29.3 77.4 89.7 2.6 86.8\\nMacaroni1 8.1 13.4 34.3 93.2 18.1 89.8\\nMacaroni2 20.9 18.4 21.4 82.3 2.7 84.2\\nPcb1 11.7 12.5 26.3 87.5 0.1 81.7\\nPcb2 12.8 13.9 37.2 75.6 0.7 78.9\\nPcb3 31.7 23.6 56.1 77.8 0.0 77.1\\nPcb4 17.1 20.3 80.4 86.8 0.0 91.3\\nPipe fryum 16.7 6.0 82.3 90.9 0.6 96.8\\nMean 14.8 17.3 56.8 86.8 3.8 87.0\\nTable 16: Fine-grained data-subset-wise performance comparison (AUROC) for anomaly classifica-\\ntion on VisA.\\nObject name CLIP CLIP-AC WinCLIP V AND CoOp AnomalyCLIP\\nCandle 37.9 33.0 95.4 83.8 46.2 79.3\\nCapsules 69.7 75.3 85.0 61.2 77.2 81.5\\nCashew 69.1 72.7 92.1 87.3 75.7 76.3\\nChewinggum 77.5 76.9 96.5 96.4 84.9 97.4\\nFryum 67.2 60.9 80.3 94.3 80.0 93.0\\nMacaroni1 64.4 67.4 76.2 71.6 53.6 87.2\\nMacaroni2 65 65.7 63.7 64.6 66.5 73.4\\nPcb1 54.9 43.9 73.6 53.4 24.7 85.4\\nPcb2 62.6 59.5 51.2 71.8 44.6 62.2\\nPcb3 52.2 49.0 73.4 66.8 54.4 62.7\\nPcb4 87.7 89.0 79.6 95.0 66.0 93.9\\nPipe fryum 88.8 86.4 69.7 89.9 80.1 92.4\\nMean 66.4 65.0 78.1 78.0 62.8 82.1\\nTable 17: Fine-grained data-subset-wise performance comparison (AP) for anomaly classification\\non VisA.\\nObject name CLIP CLIP-AC WinCLIP V AND CoOp AnomalyCLIP\\nCandle 42.9 40.0 95.8 86.9 52.9 81.1\\nCapsules 81.0 84.3 90.9 74.3 85.3 88.7\\nCashew 83.4 86.1 96.4 94.1 87.1 89.4\\nChewinggum 90.4 90.2 98.6 98.4 93.1 98.9\\nFryum 82.0 76.6 90.1 97.2 90.2 96.8\\nMacaroni1 56.8 58.7 75.8 70.9 52.3 86.0\\nMacaroni2 65.0 65.8 60.3 63.2 62.2 72.1\\nPcb1 56.9 48.4 78.4 57.2 36.0 87.0\\nPcb2 63.2 59.8 49.2 73.8 47.3 64.3\\nPcb3 53.0 47.6 76.5 70.7 54.8 70.0\\nPcb4 88.0 90.6 77.7 95.1 66.3 94.4\\nPipe fryum 94.6 93.7 82.3 94.8 89.7 96.3\\nMean 71.5 70.1 81.2 81.4 68.1 85.4\\nTable 18: Fine-grained data-subset-wise performance comparison (AUROC) for anomaly segmen-\\ntation on MPDD.\\nObject name CLIP CLIP-AC WinCLIP V AND CoOp AnomalyCLIP\\nBracket black 85.3 86.4 57.8 96.3 9.3 95.7\\nBracket brown 26.9 31.5 72.2 86.2 20.2 94.4\\nBracket white 83.5 77.4 79.5 99.0 8.3 99.8\\nConnector 56.5 52.9 79.0 90.6 7.6 97.2\\nMetal plate 64.3 52.5 92.6 93.1 14.1 93.8\\nTubes 56.4 51.5 77.6 99.1 33.2 98.1\\nMean 62.1 58.7 76.4 94.1 15.4 96.5\\nTable 19: Fine-grained data-subset-wise performance comparison (PRO) for anomaly segmentation\\non MPDD.\\nObject name CLIP CLIP-AC WinCLIP V AND CoOp AnomalyCLIP\\nBracket black 62.6 58.9 43 89.7 1.5 85.2\\nBracket brown 2.8 4.0 25.0 70.3 0.4 77.7\\nBracket white 47.9 41.6 57.6 93.1 0.0 98.8\\nConnector 22.8 20.2 44.6 74.5 0.0 89.8\\nMetal plate 31.5 27.0 78.2 74.5 0.2 86.9\\nTubes 30.4 22.9 44.7 96.9 11.5 93.6\\nMean 33.0 29.1 48.9 83.2 2.3 88.7\\n22'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 22}, page_content='Published as a conference paper at ICLR 2024\\n0.0 0.5 1.0carpet\\nNormal\\nAnomaly\\n0.0 0.5 1.0bottle\\nNormal\\nAnomaly\\n0.0 0.5 1.0hazelnut\\nNormal\\nAnomaly\\n0.0 0.5 1.0leather\\nNormal\\nAnomaly\\n0.0 0.5 1.0cable\\nNormal\\nAnomaly\\n0.0 0.5 1.0capsule\\nNormal\\nAnomaly\\n0.0 0.5 1.0grid\\nNormal\\nAnomaly\\n0.0 0.5 1.0pill\\nNormal\\nAnomaly\\n0.0 0.5 1.0transistor\\nNormal\\nAnomaly\\n0.0 0.5 1.0metal_nut\\nNormal\\nAnomaly\\n0.0 0.5 1.0screw\\nNormal\\nAnomaly\\n0.0 0.5 1.0toothbrush\\nNormal\\nAnomaly\\n0.0 0.5 1.0zipper\\nNormal\\nAnomaly\\n0.0 0.5 1.0tile\\nNormal\\nAnomaly\\n0.0 0.5 1.0wood\\nNormal\\nAnomaly\\nFigure 9: Similarity scores of CLIP on MVTec AD. Each sub-figure represents the visualization of\\none object.\\nTable 20: Fine-grained data-subset-wise performance comparison (AUROC) for anomaly classifica-\\ntion on MPDD.\\nObject name CLIP CLIP-AC WinCLIP V AND CoOp AnomalyCLIP\\nBracket black 32.4 32.8 41.5 66.1 36.9 67.3\\nBracket brown 50.9 57.9 48.6 64.0 43.9 62.2\\nBracket white 45.4 42.6 40.2 79.6 48.9 64.9\\nConnector 75 76.2 79.3 78.8 38.3 86.9\\nMetal plate 34.9 54.8 93.4 53.8 77.0 85.2\\nTubes 87.3 72.8 78.7 95.9 85.4 95.5\\nMean 54.3 56.2 63.6 73.0 55.1 77.0\\nTable 21: Fine-grained data-subset-wise performance comparison (AP) for anomaly classification\\non MPDD.\\nObject name CLIP CLIP-AC WinCLIP V AND CoOp AnomalyCLIP\\nBracket black 47.8 48.6 56.9 71.7 50.0 72.9\\nBracket brown 66.2 72.0 69.5 79.0 65.7 80.8\\nBracket white 51.2 47.3 45.1 82.3 57.5 68.5\\nConnector 62.2 61.4 61.3 71.8 26.4 76.8\\nMetal plate 70.6 78.5 97.6 78.3 92.0 94.7\\nTubes 94.4 88.2 89.1 98.1 93.6 98.1\\nMean 65.4 66.0 69.9 80.2 64.2 82.0\\n23'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 23}, page_content='Published as a conference paper at ICLR 2024\\n0.00 0.25 0.50 0.75 1.00candle\\nNormal\\nAnomaly\\n0.00 0.25 0.50 0.75 1.00capsules\\nNormal\\nAnomaly\\n0.00 0.25 0.50 0.75 1.00cashew\\nNormal\\nAnomaly\\n0.00 0.25 0.50 0.75 1.00chewinggum\\nNormal\\nAnomaly\\n0.00 0.25 0.50 0.75 1.00fryum\\nNormal\\nAnomaly\\n0.00 0.25 0.50 0.75 1.00macaroni1\\nNormal\\nAnomaly\\n0.00 0.25 0.50 0.75 1.00macaroni2\\nNormal\\nAnomaly\\n0.00 0.25 0.50 0.75 1.00pcb1\\nNormal\\nAnomaly\\n0.00 0.25 0.50 0.75 1.00pcb2\\nNormal\\nAnomaly\\n0.00 0.25 0.50 0.75 1.00pcb3\\nNormal\\nAnomaly\\n0.00 0.25 0.50 0.75 1.00pcb4\\nNormal\\nAnomaly\\n0.00 0.25 0.50 0.75 1.00pipe_fryum\\nNormal\\nAnomaly\\nFigure 10: Similarity scores of CLIP on VisA. Each sub-figure represents the visualization of one\\nobject.\\n0.0 0.5 1.0carpet\\nNormal\\nAnomaly\\n0.0 0.5 1.0bottle\\nNormal\\nAnomaly\\n0.0 0.5 1.0hazelnut\\nNormal\\nAnomaly\\n0.0 0.5 1.0leather\\nNormal\\nAnomaly\\n0.0 0.5 1.0cable\\nNormal\\nAnomaly\\n0.0 0.5 1.0capsule\\nNormal\\nAnomaly\\n0.0 0.5 1.0grid\\nNormal\\nAnomaly\\n0.0 0.5 1.0pill\\nNormal\\nAnomaly\\n0.0 0.5 1.0transistor\\nNormal\\nAnomaly\\n0.0 0.5 1.0metal_nut\\nNormal\\nAnomaly\\n0.0 0.5 1.0screw\\nNormal\\nAnomaly\\n0.0 0.5 1.0toothbrush\\nNormal\\nAnomaly\\n0.0 0.5 1.0zipper\\nNormal\\nAnomaly\\n0.0 0.5 1.0tile\\nNormal\\nAnomaly\\n0.0 0.5 1.0wood\\nNormal\\nAnomaly\\nFigure 11: Similarity scores of AnomalyCLIP on MVTec AD. Each sub-figure represents the visu-\\nalization of one object.\\n24'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 24}, page_content='Published as a conference paper at ICLR 2024\\n0.0 0.2 0.4 0.6 0.8 1.0candle\\nNormal\\nAnomaly\\n0.0 0.2 0.4 0.6 0.8 1.0capsules\\nNormal\\nAnomaly\\n0.0 0.2 0.4 0.6 0.8 1.0cashew\\nNormal\\nAnomaly\\n0.0 0.2 0.4 0.6 0.8 1.0chewinggum\\nNormal\\nAnomaly\\n0.0 0.2 0.4 0.6 0.8 1.0fryum\\nNormal\\nAnomaly\\n0.0 0.2 0.4 0.6 0.8 1.0macaroni1\\nNormal\\nAnomaly\\n0.0 0.2 0.4 0.6 0.8 1.0macaroni2\\nNormal\\nAnomaly\\n0.0 0.2 0.4 0.6 0.8 1.0pcb1\\nNormal\\nAnomaly\\n0.0 0.2 0.4 0.6 0.8 1.0pcb2\\nNormal\\nAnomaly\\n0.0 0.2 0.4 0.6 0.8 1.0pcb3\\nNormal\\nAnomaly\\n0.0 0.2 0.4 0.6 0.8 1.0pcb4\\nNormal\\nAnomaly\\n0.0 0.2 0.4 0.6 0.8 1.0pipe_fryum\\nNormal\\nAnomaly\\nFigure 12: Similarity scores of AnomalyCLIP on VisA. Each sub-figure represents the visualization\\nof one object.\\nFigure 13: Anomaly score maps for the data subset, hazelnut, in MVTec AD. The first row rep-\\nresents the input, and we circle the anomaly regions in the second row. The last row presents the\\nsegmentation results from AnomalyCLIP.\\nFigure 14: Anomaly score maps for the data subset, pill, in MVTec AD. The first row represents the\\ninput, and we circle the anomaly regions in the second row. The last row presents the segmentation\\nresults from AnomalyCLIP.\\n25'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 25}, page_content='Published as a conference paper at ICLR 2024\\nFigure 15: Anomaly score maps for the data subset, metal nut, in MVTec AD. The first row rep-\\nresents the input, and we circle the anomaly regions in the second row. The last row presents the\\nsegmentation results from AnomalyCLIP.\\nFigure 16: Anomaly score maps for the data subset, capsule, in MVTec AD. The first row represents\\nthe input, and we circle the anomaly regions in the second row. The last row presents the segmenta-\\ntion results from AnomalyCLIP.\\nFigure 17: Anomaly score maps for the data subset, screw, in MVTec AD. The first row represents\\nthe input, and we circle the anomaly regions in the second row. The last row presents the segmenta-\\ntion results from AnomalyCLIP.\\nFigure 18: Anomaly score maps for the data subset candle. The first row represents the input, and\\nwe circle the anomaly regions in the second row. The last row presents the segmentation results\\nfrom AnomalyCLIP.\\n26'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 26}, page_content='Published as a conference paper at ICLR 2024\\nFigure 19: Anomaly score maps for the data subset chewinggum. The first row represents the input,\\nand we circle the anomaly regions in the second row. The last row presents the segmentation results\\nfrom AnomalyCLIP.\\nFigure 20: Anomaly score maps for the data subset capusle. The first row represents the input, and\\nwe circle the anomaly regions in the second row. The last row presents the segmentation results\\nfrom AnomalyCLIP.\\nFigure 21: Anomaly score maps for the data subset cashew. The first row represents the input, and\\nwe circle the anomaly regions in the second row. The last row presents the segmentation results\\nfrom AnomalyCLIP.\\nFigure 22: Anomaly score maps for the data subset pcb. The first row represents the input, and we\\ncircle the anomaly regions in the second row. The last row presents the segmentation results from\\nAnomalyCLIP.\\n27'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 27}, page_content='Published as a conference paper at ICLR 2024\\nFigure 23: Anomaly score maps for the data subset pip fryum. The first row represents the input,\\nand we circle the anomaly regions in the second row. The last row presents the segmentation results\\nfrom AnomalyCLIP.\\nFigure 24: Similarity scores for the data subset bracket. The first row represents the input, and we\\ncircle the anomaly regions in the second row. The last row presents the segmentation results from\\nAnomalyCLIP.\\nFigure 25: Anomaly score maps for the data subset metal plate. The first row represents the input,\\nand we circle the anomaly regions in the second row. The last row presents the segmentation results\\nfrom AnomalyCLIP.\\nFigure 26: Anomaly score maps for the data subset tube. The first row represents the input, and we\\ncircle the anomaly regions in the second row. The last row presents the segmentation results from\\nAnomalyCLIP.\\n28'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 28}, page_content='Published as a conference paper at ICLR 2024\\nFigure 27: Anomaly score maps for the data subset grid. The first row represents the input, and we\\ncircle the anomaly regions in the second row. The last row presents the segmentation results from\\nAnomalyCLIP.\\nFigure 28: Anomaly score maps for the data subset leather. The first row represents the input, and\\nwe circle the anomaly regions in the second row. The last row presents the segmentation results\\nfrom AnomalyCLIP.\\nFigure 29: Anomaly score maps for the data subset carpet. The first row represents the input, and we\\ncircle the anomaly regions in the second row. The last row presents the segmentation results from\\nAnomalyCLIP.\\nFigure 30: Anomaly score maps for the data subset tile. The first row represents the input, and we\\ncircle the anomaly regions in the second row. The last row presents the segmentation results from\\nAnomalyCLIP.\\n29'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 29}, page_content='Published as a conference paper at ICLR 2024\\nFigure 31: Anomaly score maps for the data subset wood. The first row represents the input, and we\\ncircle the anomaly regions in the second row. The last row presents the segmentation results from\\nAnomalyCLIP.\\nFigure 32: Anomaly score maps for the data subset zipper. The first row represents the input, and we\\ncircle the anomaly regions in the second row. The last row presents the segmentation results from\\nAnomalyCLIP.\\nFigure 33: Similarity scores for the data subset skin.\\nFigure 34: Anomaly score maps for the data subset thyroid. The first row represents the input, and\\nwe circle the anomaly regions in the second row. The last row presents the segmentation results\\nfrom AnomalyCLIP.\\n30'),\n",
       " Document(metadata={'source': 'clip.pdf', 'page': 30}, page_content='Published as a conference paper at ICLR 2024\\nFigure 35: Anomaly score maps for the data subset colon. The first row represents the input, and we\\ncircle the anomaly regions in the second row. The last row presents the segmentation results from\\nAnomalyCLIP.\\nFigure 36: Anomaly score maps for the data subset brain. The first row represents the input, and we\\ncircle the anomaly regions in the second row. The last row presents the segmentation results from\\nAnomalyCLIP.\\n31')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader=PyPDFLoader('clip.pdf')\n",
    "\n",
    "docs=loader.load()\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursively split text by charachters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Published as a conference paper at ICLR 2024\n",
      "ANOMALY CLIP: O BJECT -AGNOSTIC PROMPT LEARN -\n",
      "ING FOR ZERO-SHOT ANOMALY DETECTION\n",
      "Qihang Zhou1∗, Guansong Pang2∗, Yu Tian3, Shibo He1†, Jiming Chen1†\n",
      "1Zhejiang University2Singapore Management University3Harvard University\n",
      "1{zqhang, s18he, cjm }@zju.edu.cn2gspang@smu.edu.sg\n",
      "3ytian11@meei.harvard.edu\n",
      "ABSTRACT\n",
      "Zero-shot anomaly detection (ZSAD) requires detection models trained using aux-' metadata={'source': 'clip.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "final_documents=text_splitter.split_documents(docs)\n",
    "print(final_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Published as a conference paper at ICLR 2024\n",
      "ANOMALY CLIP: O BJECT -AGNOSTIC PROMPT LEARN -\n",
      "ING FOR ZERO-SHOT ANOMALY DETECTION\n",
      "Qihang Zhou1∗, Guansong Pang2∗, Yu Tian3, Shibo He1†, Jiming Chen1†\n",
      "1Zhejiang University2Singapore Management University3Harvard University\n",
      "1{zqhang, s18he, cjm }@zju.edu.cn2gspang@smu.edu.sg\n",
      "3ytian11@meei.harvard.edu\n",
      "ABSTRACT\n",
      "Zero-shot anomaly detection (ZSAD) requires detection models trained using aux-' metadata={'source': 'clip.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(final_documents[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
